{"pages":[],"posts":[{"title":"Unity Shader 入门（二）：Shader介绍","text":"什么是Shader？Shader（着色器）：是渲染管线上的一小段程序，它负责将输入的Mesh（网格）以指定的方式和输入的贴图、颜色等组合作用然后输出。Shader开发者要做的就是根据输入，进行计算变换，产生输出。shader大体上可以分为两类： 顶点着色器（Vertex Shader） 片元着色器（Fragment Shader） 而在Unity Shader中分为三类： Surface Shaders （表面着色器）：是Unity对Vertex/Fragment Shader的一层包装，可以以极少的代码来完成不同的光照模型与不同平台下需要考虑的事情；缺点是能够实现的效果不如片段着色器来的多。 Vertex/Fragment Shaders （顶点/片断着色器） Fixed Function Shaders （固定管线着色器）：已被淘汰 什么是着色语言HLSL、GLSL、Cg？上一篇讲到了很多可编程的着色阶段（例如顶点着色器、片元着色器等等）。之所以称为可编程的着色阶段就是因为可以用一种特定的语言来编写程序，也就是着色语言（Shading language），以下几种常见的着色语言： 名称 API 优点 缺点 HLSL（High Level Shading Language） Direct3D 因为是由微软控制的着色器编译，因此即使是使用了不同的硬件，同一个着色器编译下的结果也都一样 支持HLSL的平台有限，几乎都是微软的产品（因为别的平台上没有相应的编译器） GLSL（OpenGL Shading Language） OpenGL 因为没有提供着色编译器，而是由显卡驱动来完成着色器的编译工作，因此具有优秀的跨平台性 编译的结果取决于硬件提供商（GLSL是依赖硬件而不是操作系统层次的） Cg（C for Graphic） / Cg语言是 OpenGL和 DirectX 的上层，会根据平台的不同编译成不同的中间语言，即 Cg 程序是运行在 OpenGL 和 DirectX 标准顶点和像素着色的基础上的，因此具有真正的跨平台性。因为NVIDIA和微软的合作使得 Cg 和 HLSL 语法非常相像 可能无法发挥出 OpenGL 的最新特性 什么是ShaderLab？在Unity中，所有的Shader都是使用ShaderLab来编写的，从结构上来说，它定义了显示一个材质所需要的所有东西，而不仅仅是着色器代码，我们先来看一下ShaderLab的结构 一个shader包含多个属性（Properties)，然后是一个或多个的子着色器（SubShader)，在实际运行中，哪一个子着色器被使用是由运行的平台所决定的。每一个子着色器中包含一个或者多个的Pass。在计算着色时，平台先选择最优先可以使用的着色器，然后依次运行其中的Pass，然后得到输出的结果。最后指定一个FallBack，用来处理所有Subshader都不能运行的情况,一般FallBack的都是平台已经定义好的shader。 ShaderLab和着色语言的关系对于表面着色器和顶点着色器我们可以在 ShaderLab 的 Pass 中的 CGPROGRAM 和 ENDCG 之间嵌套Cg/HLSL。或者在 GLSLPROGRAM 和 ENDGLSL 之间嵌套 GLSL。 12345678Pass { CGPROGRAM //一些编译指令，例如： #pragma vertex vert #pragma fragment frag //Cg代码 ENDCG } ShaderLab的模板当我们打开Unity，然后在Project面板点击右键，依次从中选择Create/Shader/…发现会有很多的可选项 Standard Surface Shader：标准表面着色器，是一种基于物理的着色系统（使用了Physically Based Rendering（简称PBR）技术，即基于物理的渲染技术），以模拟现实真实的方式来模拟材质与灯光之间的关系，可以很轻易的表现出各种金属反光效果，同时此种Shader的书写逻辑也更符合人类的思维模式。 Unlit Shader：Vertex/Fragment Shader,也就是最基本的顶点片断着色器，不受光照影响的Shader，多用于特效、UI上的效果制作。 Image Effect Shader：也是顶点片断着色器，只不过是针对后处理而定制的模版，例如调色、景深、模糊等，这些基于最终整个屏幕画面而进行再处理的Shader就是后处理。 Compute Shader：Compute Shader是运行在图形显卡上的一段程序，独立于常规渲染管线之外的，它可以直接将GPU作为并行处理器加以利用，从而使GPU不仅具有3D渲染能力，还具有其他的运算能力。 Shader Variant Collection：Shader变体收集器，在上面创建的时候，你会发现Shader Variant Collection与以上四个是被隔开的，就是因为这个与它们不一样，它不是制作Shader的模版，而只是对Shader变体进行打包用的容器。 注：以上的Standard Surface Shader、Unlit Shader、Image Effect Shader仅仅只是Unity为了方便我们书写而内置的几个模版，你完全可以建一个Unlit Shader，然后将其改成Surface Shader,同样也可以将一个Standard Surface Shader改成顶点片断着色器。 Shader和Material的关系由于在Unity中Shader就是运行在图形显卡上的一段包含指令的代码，所以我们需要再创建一个材质来关联它，这样才能把材质赋给场景中的物体来实现我们想要的效果。总结一下Shader与材质的关系： 一个Shader可以与无数个材质关联。 一个材质同一时刻只能关联于一个Shader。（但是我们可以通过代码去动态改变材质所关联的Shader） 材质可以赋与模型，但是Shader不行。 材质就像是Shader的实例，每个材质都可以参数不一样呈现不同的效果，但是当Shader改变时，关联它的所有材质都会相应的改变。","link":"/2019/04/29/shader-learning2/"},{"title":"Unity Shader 入门（五）：透明效果知识储备","text":"前言透明渲染是是图形学里面的常见问题之一，对于渲染算法，可以大致分为基于光和基于视图的效果。基于光的效果是指物体使得灯光衰减或改变方向，从而导致场景中的其他物体以不同方式被照明和渲染的效果。基于视图的效果是指在其中渲染半透明对象本身的效果。 透明渲染方法以下两种方法是比较常用的透明渲染方法： Screen-Door Transparency 方法基本思想是用棋盘格填充模式来绘制透明多边形；也就是说，以每隔一个像素绘制一点方式的来绘制一个多边形，这样会使在其后面的物体部分可见，通常情况下，屏幕上的像素比较紧凑，所以这种棋盘格的绘制方式并不会露馅。同样的想法也用于剪切纹理的抗锯齿边缘，但是在子像素级别中的，这是一种称为 alpha 覆盖（alpha to coverage）的特征。screen-door transparency 方法的优点就是简单，可以在任何时间任何顺序绘制透明物体，并不需要特殊的硬件支持（只要支持填充模式）。缺点是透明度效果仅在 50% 时最好，且屏幕的每个区域中只能绘制一个透明物体。 Alpha 混合（Alpha Blending）方法这个方法比较常见，其实就是按照 Alpha 混合向量的值来混合源像素和目标像素。当在屏幕上绘制某个物体时，与每个像素相关联的值有 RGB 颜色、Z 缓冲深度值，以及另外一个成分 alpha 分量，这个 alpha 值也可以根据需要生成并存储，它描述的是给定像素的对象片段的不透明度的值。alpha 为 1.0 表示对象不透明，完全覆盖像素所在区域；0.0 表示像素完全透明。为了使对象透明，在现有场景的上方，以小于 1 的透明度进行绘制即可。每个像素将从渲染管线接收到一个 RGBA 结果，并将这个值和原始像素颜色相混合。注意透明度混合要关闭深度写入。这是因为：假如一个半透明物体在一个不透明物体的前面，如果开启深度写入的话，距离摄像机更远的不透明物体就会被剔除，但是依照常理我们是可以透过半透明的物体看到不透明的物体。但是这就破坏了深度缓冲的机制，这是非常不好但是不得不做的折中方法，也因此使得渲染顺序变得非常重要。（注意：关闭深度写入，但是没有关闭深度测试）用公式来表明即：$$c_0 = \\alpha_s c_s + (1 - \\alpha_s)c_d$$ 其中 $c_s$ 是透明物体的颜色；$\\alpha_s$ 是物体的透明度；$c_d$ 是混合之前的颜色；$c_0$ 是最终的结果颜色。 透明排序我们可以不关心不透明物体的渲染顺序，因为在深度测试中就可以测试出物体离摄像机的距离再判断是否写入颜色缓冲。但是对于不透明物体，就没这么简单了，一个很自然的问题就是：如果场景中有非常多的物体，彼此之间有互相遮挡的情况，要将半透明物体正确地渲染到场景中，通常需要对物体进行排序。下面介绍几种常用的透明排序方法。 深度缓存（Z-Buffer）Z-Buffer 也称深度缓冲。在计算机图形学中，深度缓冲是在三维图形中处理图像深度坐标的过程，这个过程通常在硬件中完成，它也可以在软件中完成，它是可见性问题的一个解决方法（可见性问题是确定渲染场景中哪部分可见、哪部分不可见的问题）。 Z-buffer 的限制是每像素只存储一个对象，如果一些透明对象与同一个像素重叠，那么单独的 Z-buffer 就不能存储。这个问题可以通过改变加速器架构来解决的，比如用 A-buffer。A-buffer 具有 深度像素（deep pixels），其可以在单个像素中存储一系列呈现在所有对象之后被解析为单个像素颜色的多个片段。但需注意，Z-buffer 是市场的主流选择。 画家算法（Painter’s Algorithm）画家算法也称优先填充算法，效率虽然较低，但还是可以有效处理透明排序的问题。其基本思想是按照画家在绘制一幅画作时，首先绘制距离较远的场景，然后用绘制距离较近的场景覆盖较远的部分的思想。画家算法首先将场景中的多边形根据深度进行排序，然后按照顺序进行描绘。这种方法通常会将不可见的部分覆盖，这样就可以解决可见性问题。 加权平均值算法（Weighted Average）使用简单的透明混合公式来实现无序透明渲染的算法，它通过扩展透明混合公式，来实现无序透明物件的渲染，从而得到一定程度上逼真的结果。 深度剥离算法（Depth Peeling）深度剥离是一种对深度值进行排序的技术。它的原理比较直观，标准的深度检测使场景中的 Z 值最小的点输出到屏幕上，就是离我们最近的顶点。但还有离我们第二近的顶点，第三近的顶点存在。要想显示它们，可以用多遍渲染的方法。第一遍渲染时，按照正常方式处理，这样就得到了离我们最近的表面中的每个顶点的 z 值。在第二遍渲染时，把现在每个顶点深度值和刚才的那个深度值进行比较，凡是小于等于第一遍得到的 z 值，把它们剥离，后面的过程依次类推即可。 解决方案我们考虑两种情况： 既有半透明物体也有不透明物体：我们先渲染所有的不透明物体再渲染半透明物体 全是半透明物体：开启深度测试，关闭深度写入的情况下将半透明物体按照距离摄像机的远近从后往前渲染。 这里有一个小问题，深度缓冲中的值是像素级别的，而一个半透明物体很可能有非常多个像素，这么一来每一个像素的深度值都可能不一样，以此会产生循环遮挡的情况。 为了规避上面的问题，常常会把大的模型分割成小的几块，这样即使出现渲染错误，也不会出现太出格的结果。 Unity设置的渲染序列之前的查看 shader 我们曾经见过这样的语句 1Tags { \"RenderType\"=\"Opaque\" } 我们可以用Queue标签来决定我们的模型是怎么渲染的。 队列名称 队列索引 索引描述 Background 1000 最早被渲染的队列，一般绘制背景元素 Geometry 2000 默认渲染队列，不透明物体渲染队列 AlphaTest 2450 需要透明度测试的物体在这个队列渲染 Transparent 3000 使用透明度混合的物体在这个队列渲染 Overlay 4000 最后被渲染的物体在这个队列，一般用于叠加效果 代码设置如果我们想要通过透明度混合来实现半透明效果，代码如下 1234567SubShader { Tags { \"RenderType\"=\"Transparent\" } Pass { ZWrite Off ······ } } ZWrite Off 意味者关闭深度写入，或者可以： 123456SubShader { Tags { \"RenderType\"=\"Transparent\" } ZWrite Off ······ Pass { } } 这样表示这个SubShader下的所有Pass都会关闭深度写入 结语下一篇我们就运用这些理论开始写第一个半透明的shader","link":"/2019/05/03/shader-learning5/"},{"title":"Unity Shader 入门（七）：模型描边Shader","text":"前言前面几篇我们写了几个边缘发光的shader，另外一个类似功能的就是模型描边，和边缘发光不同的地方在于，描边是在原有模型的基础上，添加一圈的外框。 老规矩还是来看一下效果图： 实现原理利用两个Pass来绘制： 第一个Pass将所有表面延展模型，挤出一点点并只输出描边的颜色 第二个Pass就是进行正常的着色工作 Shader代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374Shader \"Kurong/NPR/Outline\"{ Properties { _Color (\"Color\", Color) = (1,1,1,1) _MainTex (\"Texture\", 2D) = \"white\" {} _OutlineRange (\"Outline Range\", Range(0,0.5)) = 0.1 _OutlineColor(\"Outline Color\", Color) = (1,1,1,1) } SubShader { Pass { Tags { \"RenderType\"=\"Opaque\" } LOD 200 ZWrite Off CGPROGRAM #pragma vertex vert #pragma fragment frag #include \"UnityCG.cginc\" float _OutlineRange; float4 _OutlineColor; struct a2v { float4 vertex : POSITION; }; struct v2f { float4 pos : SV_POSITION; }; v2f vert (a2v v) { v.vertex.xyz += _OutlineRange * normalize(v.vertex.xyz); v2f o; o.pos = UnityObjectToClipPos(v.vertex); return o; } fixed4 frag (v2f v) : Color { return _OutlineColor; } ENDCG } CGPROGRAM #pragma surface surf Standard fullforwardshadows sampler2D _MainTex; fixed4 _Color; struct Input { float2 uv_MainTex; }; void surf (Input IN, inout SurfaceOutputStandard o) { fixed4 c = tex2D(_MainTex, IN.uv_MainTex) * _Color; o.Albedo = c.rgb; o.Alpha = c.a; } ENDCG } FallBack \"Diffuse\"} 第一个Pass1234567891011121314151617181920212223242526272829303132333435363738394041Pass{ Tags { \"RenderType\"=\"Opaque\" } LOD 200 ZWrite Off CGPROGRAM #pragma vertex vert #pragma fragment frag #include \"UnityCG.cginc\" float _OutlineRange; float4 _OutlineColor; struct a2v { float4 vertex : POSITION; }; struct v2f { float4 pos : SV_POSITION; }; v2f vert (a2v v) { v.vertex.xyz += _OutlineRange * normalize(v.vertex.xyz); v2f o; o.pos = UnityObjectToClipPos(v.vertex); return o; } fixed4 frag (v2f v) : Color { return _OutlineColor; } ENDCG} 结构体定义123456789struct a2v{ float4 vertex : POSITION;};struct v2f{ float4 pos : SV_POSITION;}; 经过上一篇的学习，应该对结构体比较熟悉了： a2v ：包含顶点着色器要的模型数据 float4 vertex : POSITION; 用模型顶点的坐标填充vertex变量。 v2f ：用于顶点着色器和片元着色器之间传递信息 float4 pos : SV_POSITION; 用裁剪空间的位置信息填充pos变量 顶点着色器1234567v2f vert (a2v v){ v.vertex.xyz += _OutlineRange * normalize(v.vertex.xyz); v2f o; o.pos = UnityObjectToClipPos(v.vertex); return o;} v.vertex.xyz += _OutlineRange * normalize(v.vertex.xyz); 将顶点的xyz单位化后和定义的 _OutlineRange 相乘，使得模型挤出 _OutlineRange 的距离 UnityObjectToClipPos(v.vertex); 将模型空间的顶点信息转换到裁剪空间中的位置信息，然后将信息存储在o.pos中。 片元着色器1234fixed4 frag (v2f v) : Color{ return _OutlineColor;} 直接输出描边颜色 第二个Pass123456789101112131415161718CGPROGRAM#pragma surface surf Standard fullforwardshadowssampler2D _MainTex;fixed4 _Color;struct Input{ float2 uv_MainTex;};void surf (Input IN, inout SurfaceOutputStandard o){ fixed4 c = tex2D(_MainTex, IN.uv_MainTex) * _Color; o.Albedo = c.rgb; o.Alpha = c.a;}ENDCG 这里注意：surface shader是对vertex shader 和 fragment shader的更高一层的包装，不需要我们再去编写Pass了，直接编写 CGPROGRAM 。","link":"/2019/05/05/shader-learning7/"},{"title":"Unity Shader 入门（六）：边缘发光透明版","text":"前言之前我们写过一个边缘发光的Shader，这一次我们来写这个的升级版：透明物体的边缘发光。 效果图首先我们还是来看一下效果图： Shader代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364Shader \"Kurong/Rim/RimLightingTranslucent\"{ Properties { _Color (\"Color\", Color) = (1,1,1,1) _AlphaRange(\"Alpha Range\",Range(0,1)) = 0 _RimColor(\"Rim Color\",Color) = (1,1,1,1) } SubShader { Tags { \"RenderType\"=\"Opaque\" \"Queue\" = \"Transparent\" \"RenderType\" = \"Transparent\"} LOD 200 ZWrite Off Blend SrcAlpha OneMinusSrcAlpha Pass { CGPROGRAM #pragma vertex vert #pragma fragment frag #include \"UnityCG.cginc\" sampler2D _NormalMap; float _AlphaRange; float4 _RimColor; fixed4 _Color; struct a2v { float4 vertex : POSITION; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float3 normalDir : TEXCOORD0; float3 worldPos : TEXCOORD1; }; v2f vert (a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.normalDir = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld,v.vertex).xyz; return o; } fixed4 frag (v2f v) : COLOR { float3 viewDir = normalize(_WorldSpaceCameraPos - v.worldPos); float normalDotViewDir = saturate(dot(normalize(v.normalDir),viewDir)); fixed3 diffuse = normalDotViewDir *_Color; return fixed4(diffuse + _RimColor ,(1 - normalDotViewDir) * (1 - _AlphaRange) + _AlphaRange); } ENDCG } } FallBack \"Diffuse\"} 透明度混合上一篇我们了解了透明度混合的原理以及一些透明效果知识储备，而Unity中，为了进行透明度混合，我们需要用到 Blend 命令： 语法 描述 Blend Off 关闭混合（这是默认的状态） Blend SrcFactor DstFactor 开启混合，该片元产生的颜色 SrcFactor. 已存在于屏幕的颜色 DstFactor，然后将两者叠加在一起存入颜色缓冲。 Blend SrcFactor DstFactor, SrcFactorA DstFactorA 原理同上，不过使用了不同的混合因子 BlendOp Op 不同于上面的颜色混合，而是使用Blend Operation来对它们进行操作 BlendOp OpColor, OpAlpha 原理同上，不过采用不同的Blend Operation来操作Color和Alpha的通道 混合因子： 名称 描述 One 因子为1，表示让源颜色或者目标颜色通过 Zero 因子为0，用来删除源颜色或者目标颜色 SrcColor 因子为源颜色 SrcAlpha 因子为源颜色的透明度 DstColor 因子为目标颜色 DstAlpha 因子为目标颜色的透明度 OneMinusSrcColor 因子为 (1 - 源颜色) 的值 OneMinusSrcAlpha 因子为 (1 - 源颜色的透明度) 的值 OneMinusDstColor 因子为 (1 - 目标颜色) 的值 OneMinusDstAlpha 因子为 (1 - 目标颜色的透明度) 的值 此时我们再来看上面这一块代码： 12345Tags{ Tags { \"RenderType\"=\"Opaque\" \"Queue\" = \"Transparent\" \"RenderType\" = \"Transparent\"} LOD 200 ZWrite Off Blend SrcAlpha OneMinusSrcAlpha 这里有一些新的知识：之前提过半透明物体的渲染序列要设置成 “Queue”=”Transparent” ,而 “RenderType”=”Transparent” 表示我们使用了透明度混合。通常一个半透明的Shader Tags都包含这三条： 1Tags { \"RenderType\"=\"Opaque\" \"Queue\" = \"Transparent\" \"RenderType\" = \"Transparent\"} 我们在上一篇介绍过为什么透明度混合需要关闭深度写入 1ZWrite Off 这里我们将源颜色的混合因子设置成SrcAlpha，将目标颜色的混合因子设置成 OneMinusSrcAlpha 以得到半透明效果。 1Blend SrcAlpha OneMinusSrcAlpha 结构体定义123456789101112struct a2v{ float4 vertex : POSITION; float3 normal : NORMAL;};struct v2f{ float4 pos : SV_POSITION; float3 normalDir : TEXCOORD0; float3 worldPos : TEXCOORD1;}; a2v ：包含顶点着色器要的模型数据 float4 vertex : POSITION; 用模型顶点的坐标填充vertex变量。 float3 normal : NORMAL; 用模型空间的法线方向向量填充normal变量 v2f ：用于顶点着色器和片元着色器之间传递信息 float4 pos : SV_POSITION; 用裁剪空间的位置信息填充pos变量 float3 normalDir : TEXCOORD0; 用模型的第一套纹理坐标填充normalDir变量 float3 worldPos : TEXCOORD1; 用模型的第二套纹理坐标填充worldPos变量 顶点着色器12345678v2f vert (a2v v){ v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.normalDir = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld,v.vertex).xyz; return o;} UnityObjectToClipPos(v.vertex); 将模型空间的顶点信息转换到裁剪空间中的位置信息，然后将信息存储在o.pos中。 UnityObjectToWorldNormal(v.normal); 法线从模型空间变换到世界空间中并计算物体在世界空间中的法线坐标。 mul(unity_ObjectToWorld,v.vertex).xyz; 将顶点从模型空间转换到世界空间的信息存储到worldPos变量中。 片元着色器1234567fixed4 frag (v2f v) : COLOR{ float3 viewDir = normalize(_WorldSpaceCameraPos - v.worldPos); float normalDotViewDir = saturate(dot(normalize(v.normalDir),viewDir)); fixed3 diffuse = normalDotViewDir *_Color; return fixed4(diffuse + _RimColor ,(1 - normalDotViewDir) * (1 - _AlphaRange) + _AlphaRange);} fixed4 frag( v2f v ):COLOR 我们注意到片元着色器的后面跟着 : COLOR ：这是Unity提供的Cg/HLSL语义。语义可以告诉shader数据的来源以及数据的输出。 float3 viewDir = normalize(_WorldSpaceCameraPos - v.worldPos); 这里我们用 对象在世界坐标系中的位置 减去 摄像机的世界空间位置 ，并进行逐顶点归一化，赋给视线的方向 float normalDotViewDir = saturate(dot(normal,viewDir)); 我们获得法线与视线的夹角 fixed3 diffuse = normalDotViewDir *_Color; 这里我们视线与法线的夹角和主颜色相乘。 return fixed4(diffuse + _RimColor ,(1 - normalDotViewDir) * (1 - _AlphaRange) + _AlphaRange); 最后将混合后的颜色输出。 结语描边常用于一些漫画风格的游戏场景中，能够在复杂的场景中突出被绘制的物体，下一篇我们就写物体描边的shader。","link":"/2019/05/04/shader-learning6/"},{"title":"Unity Shader 入门（三）：查看Shader","text":"新建Shader首先我们新建一个Shader，这里以 Standard Surface Shader 为例，新建之后我们打开Shader文件应该会出现如下代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253Shader \"Custom/NewSurfaceShader\"{ Properties { _Color (\"Color\", Color) = (1,1,1,1) _MainTex (\"Albedo (RGB)\", 2D) = \"white\" {} _Glossiness (\"Smoothness\", Range(0,1)) = 0.5 _Metallic (\"Metallic\", Range(0,1)) = 0.0 } SubShader { Tags { \"RenderType\"=\"Opaque\" } LOD 200 CGPROGRAM // Physically based Standard lighting model, and enable shadows on all light types #pragma surface surf Standard fullforwardshadows // Use shader model 3.0 target, to get nicer looking lighting #pragma target 3.0 sampler2D _MainTex; struct Input { float2 uv_MainTex; }; half _Glossiness; half _Metallic; fixed4 _Color; // Add instancing support for this shader. You need to check 'Enable Instancing' on materials that use the shader. // See https://docs.unity3d.com/Manual/GPUInstancing.html for more information about instancing. // #pragma instancing_options assumeuniformscaling UNITY_INSTANCING_BUFFER_START(Props) // put more per-instance properties here UNITY_INSTANCING_BUFFER_END(Props) void surf (Input IN, inout SurfaceOutputStandard o) { // Albedo comes from a texture tinted by color fixed4 c = tex2D (_MainTex, IN.uv_MainTex) * _Color; o.Albedo = c.rgb; // Metallic and smoothness come from slider variables o.Metallic = _Metallic; o.Smoothness = _Glossiness; o.Alpha = c.a; } ENDCG } FallBack \"Diffuse\"} emmm…….即使你有编程的基础也可能看的一头雾水，不过没关系，我们现在来拆解这段代码。 拆解代码接下来我们讲逐句讲解这个Shader，弄懂每一个语句的意义。 Shader名称路径1Shader \"Custom/NewSurfaceShader\" 首先这里标注了Shader的名字和路径，符号 “/“ 表示的是创建子层级，我们可以进行修改，例如这里修改为 1Shader \"MyShader/FirstShader\" 我们用这个Shader新建一个材质，可以看见Shader的层级就像我们设置的这样这里有一个注意点：如果我们把路径名称放在 Hidden下面的话，比如： 1Shader \"Hidden/MyShader/FirstShader\" 则表示在材质面板中隐藏此Shader，你将无法通过材质下拉列表中找到它。这在做一些不需暴露的Shader时很有用处，可以使Shader下拉列表更精简整洁。 Shader属性1234567Properties{ _Color (\"Color\", Color) = (1,1,1,1) _MainTex (\"Albedo (RGB)\", 2D) = \"white\" {} _Glossiness (\"Smoothness\", Range(0,1)) = 0.5 _Metallic (\"Metallic\", Range(0,1)) = 0.0} 这里是shader的属性部分：属性的格式写作如下 1[Attribute]_Name (\"Display Name\",Type) = Default Value Attribute：属性标记，用于对当前这条属性进行一些特殊的处理，此标记不是必选项，可以不添加，同时一条属性上也可以有多条属性标记。 _Name：变量名，在之后的Shader代码中都用这个名字来获取该属性的内容，在名称前一定要加上下划线。 Display Name：在Unity Inspector上显示的名字，主要起到说明解释的作用。 Type：类型，可能的type所表示的内容有以下几种： Default Value：上面类型的默认值 属性分类 属性标记 适用类型 举例 作用 HDR Color [HDR]_Color(“Color”, Color) = (1,1,0,1) 当给颜色添加了HDR后，则在材质面板中的颜色上会显示HDR的字样，HDR可以使颜色亮度的值超过1，通过这个值可以配合镜头Bloom效果做出物体泛光的视觉效果。 PowerSlider Range [PowerSlider(3)]_MyRange(“Range”,Range(0.0,1.0)) = 0.5 方便用户调节滑杆，例如有个属性值是从0-1,但在大部分情况下所用到的值都是0-0.1，同时需要更精细的在这区间进行微调。一般情况下用户难以控制微调，这个时候就可以利用PowerSlider来解决此问题。 Toggle Range [Toggle]_MyRange(“Range”,Range(0.0,1.0)) =0.5 表示开关，0代表关，1代表开 Enum Range [Enum(···)]_MyRange(“Range”,Range(0.0,1.0)) =0.5 枚举，显示下拉列表 NoScaleOffset 2D [NoScaleOffset]_MainTex(“2D”, 2D) = “white” {} 纹理贴图在材质面板中除了显示贴图槽以外默认还会显示两组Float。如果我们不希望用户去调节此参数，我们可以使用[NoScaleOffset]属性标记来将它们隐藏掉。 Normal 2D [Normal]_MainTex(“2D”, 2D) = “white” {} 添加[Normal]，来标记此属性是用来接收法线贴图的，当用户指定了非法线的贴图时会在材质面板上进行警告提示 Header 所有 [Header(I am Header)]_MyInt(“Int”,Int) = 1 在材质面板上进行标注，通常用作分类组别用 HideInInspector 所有 [HideInInspector]_MyInt(“Int”,Int) = 1 在材质面板中隐藏此条属性，在不希望暴露某条属性时可以快速将其隐藏。 类型分类 类型 说明 语法 Float 浮点数，注意浮点数值后不需要加后缀f _MyFloat(“Float”,Float) = 3.5 Int 整型数 _MyInt(“Int”,Int) = 1 Range(min,max) 一个介于最小值和最大值之间的浮点数 _MyRange(“Range”,Range(0.0,1.0)) = 0.5 Color RGBA（红绿蓝和透明度）四个量来定义的颜色 _MyColor(“Color”,Color) = (1,1,1,1) 2D 贴图信息 _My2D(“2D”,2D) = “white”{} Cube 立方纹理，由6张关联的2D贴图合在一起 _MyCube(“Cube”,Cube) = “bump”{} Vector 四维数 _MyVector(“Vector”,Vector) = (1,2,3,1) Shader TagsTags用来告诉渲染器：何时以及怎样渲染这个对象。详细内容可以查看官方文档 1Tags { \"RenderType\"=\"Opaque\" } 标签名称 标签说明 例子 Queue 控制渲染顺序，保证不透明物体在透明物体之前渲染 Tags {“Queue”=”Transparent”} RenderType 对着色器分类，例如这是渲染透明的，这是渲染不透明的 Tags {“RenderType”=”Opaque”} DisableBatching 是否对该SubShader进行批处理 Tags {“DisableBatching”=”True”} ForceNoShadowCasting 该SubShader是否会投射阴影 Tags {“ForceNoShadowCasting”=”True”} IgnoreProjector 该SubShader是否会Project影响，常用于半透明物体 Tags {“IgnoreProjector”=”True”} CanUseSpriteAtlas 该SubShader用于Sprites时，要设置成false Tags {“CanUseSpriteAtlas”=”False”} PreviewType Inspector preview上默认是圆形预设，可以改为plane或者skybox Tags {“PreviewType”=”Plane”} 这里想要着重说一下的是Queue这个标签，如果你使用Unity做过一些透明和不透明物体的混合的话，很可能已经遇到过不透明物体无法呈现在透明物体之后的情况。这种情况很可能是由于Shader的渲染顺序不正确导致的。Queue指定了物体的渲染顺序，预定义的Queue有： Background - 最早被调用的渲染，用来渲染天空盒或者背景 Geometry - 这是默认值，用来渲染非透明物体（普通情况下，场景中的绝大多数物体应该是非透明的） AlphaTest - 用来渲染经过Alpha Test的像素，单独为AlphaTest设定一个Queue是出于对效率的考虑 Transparent - 以从后往前的顺序渲染透明物体 Overlay - 用来渲染叠加的效果，是渲染的最后阶段（比如镜头光晕等特效） 这些预定义的值本质上是一组定义整数，Background = 1000， Geometry = 2000, AlphaTest = 2450， Transparent = 3000，最后Overlay = 4000。在我们实际设置Queue值时，不仅能使用上面的几个预定义值，我们也可以指定自己的Queue值，写成类似这样：”Queue” = “Transparent+100”，表示一个在Transparent之后100的Queue上进行调用。通过调整Queue值，我们可以确保某些物体一定在另一些物体之前或者之后渲染，这个技巧有时候很有用处。 LOD：Level of Detail1LOD 200 这个数值决定了我们能用什么样的Shader。当设定的LOD小于SubShader所指定的LOD时，这个SubShader就不可以用了。Unity自定义了一组LOD的数值，我们在实现自己的Shader的时候可以参考来设定自己的LOD数值，以便控制渲染。 LOD名称 数值 VertexLit及其系列 100 Decal, Reflective VertexLit 150 Diffuse 200 Diffuse Detail, Reflective Bumped Unlit, Reflective Bumped VertexLit 250 Bumped, Specular 300 Parallax 500 Parallax Specular 600 Shader主体代码终于到了最重要的部分，首先CGPROGRAM和ENDCG成对出现，表示中间包裹的是一段Cg程序，接着是一个编译指令： 1#pragma surface surf Standard fullforwardshadows 意味着我们要写一个表面Shader，并指定了光照模型，具体语法是 1#pragma surface surfaceFunction lightModel [optionalparams] surface ： 声明的是一个表面着色器 surfaceFunction ： 着色器代码的方法的名字 lightModel ： 使用的光照模型。 对应上面的编译指令：我们声明了一个表面着色器，实际的代码在 surf 函数中（在下面的Shader代码能找到该函数），使用 Standard 作为光照模型。接下来是 1sampler2D _MainTex; 我们知道在CG中，Texture（贴图）简单来说就是一块内存存储的，使用了RGBA通道，且每个通道8bits的数据。而具体地想知道像素与坐标的对应关系，以及获取这些数据，一次一次去计算内存地址或者偏移显然不可行，因此可以通过sampler2D来对贴图进行操作。一言以蔽之就是，sampler2D是GLSL中的2D贴图的类型，相应的，还有sampler1D，sampler3D，samplerCube等等格式。然后的重点是：为什么在这里需要一句对_MainTex的声明？首先之前我们已经在Properties里声明过它是贴图了 1_MainTex (\"Albedo (RGB)\", 2D) = \"white\" {} 我们用来实例的这个shader其实是由两个相对独立的块组成的，外层的属性声明，回滚等等是Unity可以直接使用和编译的ShaderLab；而现在我们是在CGPROGRAM…ENDCG这样一个代码块中，这是一段Cg程序。对于这段Cg程序，要想访问在Properties中所定义的变量的话，必须使用和之前变量相同的名字进行声明。因此这样做就是再次声明并链接了_MainTex，使得接下来的Cg程序能够使用这个变量。后面的： half _Glossiness; half _Metallic; fixed4 _Color; 都是同样的道理。回到原来的地方，下一句是: 123struct Input { float2 uv_MainTex;}; 表面着色器如果你有编程的经历，那么结构体应该很熟悉了，这一段我们结合下面的surf一起来说 1234567891011void surf (Input IN, inout SurfaceOutputStandard o) { // Albedo comes from a texture tinted by color fixed4 c = tex2D (_MainTex, IN.uv_MainTex) * _Color; o.Albedo = c.rgb; // Metallic and smoothness come from slider variables o.Metallic = _Metallic; o.Smoothness = _Glossiness; o.Alpha = c.a; } ENDCG 上文提到的 surf 函数就是对应这一段。我们看函数头输入的参数有Input IN，这个Input就对应上面的结构体。我们可以把所需要参与计算的数据都放到这个Input结构中，再传入surf函数使用；SurfaceOutputStandard 是已经定义好了里面类型输出结构。作为输入的结构体必须命名为Input，这个结构体中定义了一个float2的变量，emmmm···你可能会感到奇怪float后面跟着数字，这是什么意思呢？其实float和vec都可以在之后加入一个2到4的数字，来表示被打包在一起的2到4个同类型数。比如： 12float4 color;float3 multipliedColor = color.rgb * coordinate.x; 在这个例子里，我们声明了一个叫做 uv_MainTex 的包含两个浮点数的变量。UV mapping 的作用是将一个2D贴图上的点按照一定规则映射到3D模型上，在Cg程序中，我们有这样的约定，在一个贴图变量之前加上uv两个字母，就代表提取它的uv值。我们之后就可以在 surf 程序中直接通过访问uv_MainTex 来取得这张贴图当前需要计算的点的坐标值。接下来我们详细看surf内部的操作： 1fixed4 c = tex2D (_MainTex, IN.uv_MainTex) * _Color; 这里用到了一个tex2d函数，这是Cg程序中用来在一张贴图中对一个点进行采样的方法，返回一个float4。这个例子中用刚刚得到的 float4 和 _Color 相乘，使得这个贴图经过和颜色混合。 1o.Albedo = c.rgb; 将其颜色的rbg值赋予了输出的像素颜色 12o.Metallic = _Metallic;o.Smoothness = _Glossiness; 都是用到上头Properties中我们定义的变量来赋值材质中的 Metallic 和 smoothness 1o.Alpha = c.a; 将a值赋予透明度。至此surf介绍完毕，这个例子中shader最重要的部分就是以上这些啦！ FallBack1FallBack \"Diffuse\" 当所有上面的SubShader都不可以在目标平台上运行时，Unity就会调用这个shader，当然你也可以关闭这个选项，那就意味着如果没有显卡可以跑上面的shader，我们就不管它啦! 结语这是最简单最简单的模板shader，看到这里的你应该可以了解一些简单的shader了，可以去Unity的Surface Shader Exampless上查看一些基础shader的编写内容，下一篇我们会开始第一个shader的编写。","link":"/2019/05/01/shader-learning3/"},{"title":"Unity Shader 入门（四）：边缘发光效果","text":"前言上一节我们学习查看了第一个模板Shader，现在我们开始写第一个Shader练练手。首先我们挑一个：边缘发光效果 的shader来写，先来看一下效果图： 实现原理根据物体表面法向量和视线向量的夹角来判断是否是物体的边缘部位。夹角越大（接近垂直）说明越接近物体边缘部分，重点：向量点积运算。 具体实现先放一段实现的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142Shader \"Kurong/Rim/RimLightingOpaque\"{ Properties { _Color(\"Color\", Color) = (1,1,1,1) [Normal]_NormalMap(\"Normal Map\", 2D) = \"bump\" {} _RimColor(\"Rim Color\", Color) = (1,1,1,0.0) _RimPower(\"Rim Power\", Range(0.5,6.0)) = 1.0 } SubShader { Tags { \"RenderType\" = \"Opaque\" } LOD 200 CGPROGRAM #pragma surface surf Standard fullforwardshadows #pragma target 3.0 fixed4 _Color; sampler2D _NormalMap; float4 _RimColor; float _RimPower; struct Input { float2 uv_NormalMap; float3 viewDir; }; void surf(Input IN, inout SurfaceOutputStandard o) { o.Normal = UnpackNormal(tex2D(_NormalMap, IN.uv_NormalMap)); o.Albedo = _Color; half rim = 1 - saturate(dot(o.Normal,normalize(IN.viewDir))); o.Emission = _RimColor.rgb * pow(rim,_RimPower); } ENDCG } FallBack \"Diffuse\"} 表面着色器如果你看过上一篇的Shader介绍你应该可以大致看懂上面的代码，我们就关键部分说明一下： 1234567void surf(Input IN, inout SurfaceOutputStandard o){ o.Normal = UnpackNormal(tex2D(_NormalMap, IN.uv_NormalMap)); o.Albedo = _Color; half rim = 1 - saturate(dot(o.Normal,normalize(IN.viewDir))); o.Emission = _RimColor.rgb * pow(rim,_RimPower);} 1o.Normal = UnpackNormal(tex2D(_NormalMap, IN.uv_NormalMap)); UnpackNormal：是定义在UnityCG.cginc文件中的方法（这个文件中包含了一系列常用的CG变量以及方法，在Unity安装路径中可以找到），接受一个fixed4的输入，并将其转换为所对应的法线值。在解包得到这个值之后，将其赋给输出的Normal，这里如果有疑惑的话可以跳转下面的拓展知识。 1half rim = 1 - saturate(dot(o.Normal,normalize(IN.viewDir))); normalize 函数：为了对向量进行归一化处理（这里传入 IN.viewDir 指的是 WorldSpace View Direction，也就是当前坐标的视角方向） dot 函数：返回传入的两个参数的点积 saturate 函数：判断传入的参数是否在 0-1 之间，如果小于0，返回 0；如果大于 1，返回1 1o.Emission = _RimColor.rgb * pow(rim,_RimPower); 从 _RimColor 参数获取自发光颜色再和发光的强度混合，最终将颜色赋值给像素的Emission（发散颜色） 拓展知识一般情况下，模型面数越高，可以表现的细节越多，效果就更加真实，相应的计算量也就增大，性能下降。为了平衡这两者的矛盾，常常就使用法线贴图来解决，法线贴图就是把法线信息储存在一张图里。使用法线贴图时，通常顶点数和三角形面数只有高精度模型的十分之一不到，因此可以通过低面数模型来模拟高面数模型的效果，增加细节层次感，效果与高模相差不多，但是大大降低了模型的面数。 法线贴图的历史从上古时代来看，最早提出的是凹凸贴图（Bump Mapping），凹凸贴图的思想最早是由图形学届大牛中的大牛 Jim Blinn 提出，后来的 Normal Mapping，Parallax Mapping，Parallax Occulision Mapping，Relief Mapping 等等，均是基于同样的思想，只是考虑得越来越全面，效果也越来越逼真。 凹凸贴图 Bump Mapping凹凸贴图是指计算机图形学中在三维环境中通过纹理方法来产生表面凹凸不平的视觉效果。它主要的原理是通过改变表面光照方程的法线，而不是表面的几何法线，或对每个待渲染的像素在计算照明之前都要加上一个从高度图中找到的扰动，来模拟凹凸不平的视觉特征，如褶皱、波浪等等。 Blinn 于 1978 年提出了凹凸贴图方法。使用凹凸贴图，是为了给光滑的平面，在不增加顶点的情况下，增加一些凹凸的变化。他的原理是通过法向量的变化，来产生光影的变化，从而产生凹凸感，而顶点是没有变换的。表示凹凸效果的另一种方法是使用高度图来修改表面法线的方向。每个单色纹理值代表一个高度，所以在纹理中，白色表示高高度区域，黑色是低高度的区域（反之亦然）。示例如图： 移位贴图 Displacement Mapping移位贴图，也有人称为置换贴图，或称高度纹理贴图（Heightfield Texturing）。这种方法类似于法线贴图，移位贴图的每一个纹素中存储了一个向量，这个向量代表了对应顶点的位移。注意，此处的纹素并不是与像素一一对应，而是与顶点一一对应，因此，纹理的纹素个数与网格的顶点个数是相等的。在 VS 阶段，获取每个顶点对应的纹素中的位移向量，施加到局部坐标系下的顶点上，然后进行世界视点投影变换即可。 法线贴图 Normal Mapping法线贴图（Normal mapping）是凸凹贴图（Bump mapping）技术的一种应用，法线贴图有时也称为“Dot3（仿立体）凸凹纹理贴图”。凸凹与纹理贴图通常是在现有的模型法线添加扰动，不同的是，法线贴图要完全更新法线。与凸凹贴图类似的是，它也是用来在不增加多边形的情况下在浓淡效果中添加细节。但是凸凹贴图通常根据一个单独的灰度图像通道进行计算，而法线贴图的数据源图像通常是从更加细致版本的物体得到的多通道图像，即红、绿、蓝通道都是作为一个单独的颜色对待。简单来说，Normal Map 直接将正确的 Normal 值保存到一张纹理中去，那么在使用的时候直接从贴图中取即可。 视差贴图 Parallax Mapping凹凸贴图和法线贴图的一个问题是，凹凸的部分永远不会随视角移动，也不会相互遮挡。如果你看一个真正的砖墙，你可能看不到砖块间的灰浆. 最好让凸起的效果实际的影响在表面的每个像素点的位置上。视差贴图 Parallax Mapping，又称为 Offset Mapping，以及 virtual displacement mapping，于2001 年由 Kaneko 引入，由 Welsh 进行了改进和推广。视差贴图是通过替换渲染多边形上的顶点处的纹理坐标来实现的，而这个替换依赖于一个关于切线空间中的视角（相对于表面法线的角度）和在该点上的高度图的方程。简单来说，Parallax Mapping 利用 Height Map 进行了近似的 Texture Offset。如图： 浮雕贴图 Relief Mapping关于浮雕贴图（Relief Mapping），有人把它誉为凹凸贴图的极致。我们知道，Parallax Mapping 是针对 Normal Mapping 的改进，利用 HeightMap 进行了近似的 Texture Offset。而Relief Mapping 是精确的 Texture Offset，所以在表现力上比较完美。 parallax Mapping 能够提供比 Bump Mapping 更多的深度，尤其相比于小视角下，但是如果想提供更深的深度,Parallax Mapping 就无能为力了，Relief Mapping 则可以很好的胜任。相较于Parallax Mapping， Relief Mapping 可以实现更深的凹凸深度。浮雕贴图方法不仅更容易提供更深的深度,还可以做出自阴影和闭塞效果，当然算法也稍稍有点复杂，而如果要用一句话概括 Relief Mapping，将会是：“在 Shader 里做光线追踪”。 法线贴图的存储历史了解一下，然后我们来看一下法线贴图是怎样存储的：我们知道法线贴图中存储的是法线的方向，也就是说是一个Vector3类型的变量，刚好和图片的 RGB 格式不谋而合。但是向量是有方向的，且贴图中只能存储的都是正数，所以还需要一个映射的过程。映射在图形学中的应用很多，比如计算半兰伯特光照时就通过把（0,1）的光照区间转化到了（0.5,1）提高了光的亮度，使效果更好。在法线贴图中，可以用0代表向量中的-1，用255代表向量中的1，不过，在shader中，贴图的颜色一般也是（0,1）区间，所以，我们在计算时只需要把从法线贴图中采样得到的法线值进行映射，将其从（0,1）区间转化到（-1,1）区间。这个步骤，Unity已经为我们完成了，我们在计算法线的时候，只需要调用 UnpackNormal 这个函数就可以实现区间的重新映射。我们在 UnityCG.cginc 中查看 UnpackNormal 的源码： 12345678910111213141516171819202122232425262728inline fixed3 UnpackNormalDXT5nm (fixed4 packednormal){ fixed3 normal; normal.xy = packednormal.wy * 2 - 1; normal.z = sqrt(1 - saturate(dot(normal.xy, normal.xy))); return normal;}// Unpack normal as DXT5nm (1, y, 1, x) or BC5 (x, y, 0, 1)// Note neutral texture like \"bump\" is (0, 0, 1, 1) to work with both plain RGB normal and DXT5nm/BC5fixed3 UnpackNormalmapRGorAG(fixed4 packednormal){ // This do the trick packednormal.x *= packednormal.w; fixed3 normal; normal.xy = packednormal.xy * 2 - 1; normal.z = sqrt(1 - saturate(dot(normal.xy, normal.xy))); return normal;}inline fixed3 UnpackNormal(fixed4 packednormal){#if defined(UNITY_NO_DXT5nm) return packednormal.xyz * 2 - 1;#else return UnpackNormalmapRGorAG(packednormal);#endif} 看第一个 UnpackNormal 函数，从函数名来看为了专门解出DXT5nm格式的normal map，这种类型的normal map，只用存储法向量中的两个通道，然后解开的时候，需要计算一下，重新算出另一个向量方向。这样可以实现的原理在于，存储的向量是单位向量，长度一定的情况下，就可以通过 $sqrt(1 - x^2 - y^2)$ 来计算。其他的映射就是简单的乘2减1大法。们经常看到法线贴图是蓝紫色的，这是因为法线经常是朝着Z轴的，也就是(0,0,1)，经过上文的的公式映射后就是(0.5,0.5,1)，因此贴图偏蓝紫色。 我法线贴图中一般都存储的是切线空间，为什么不存储在世界空间或者模型空间。首先看一下世界空间，如果我们的法线贴图存储的世界空间的法线信息，我们可以直接解出法线的值，在世界空间进行计算，是最直接并且计算效率最高的做法，但是世界空间的法线贴图就跟当前环境之间耦合过大了，比如同样的两个模型，仅仅是旋转方向不同，也需要两张法线贴图，这很明显是多余的，于是就有人想出了基于模型空间的法线，基于模型空间，在计算时，把模型空间的法线转换到世界空间，虽然多了一步操作，但是同一个模型可以共用法线，不用考虑旋转等问题。但是，人们感觉模型空间的法线贴图跟模型的耦合度还是高，那就继续解耦吧，于是基于切线空间的法线贴图就诞生了。那么如何按照模型顶点的位置坐标随纹理坐标(u, v)的变化作「切线空间」呢？ $$T = (\\frac{\\partial x}{\\partial u},\\frac{\\partial y}{\\partial u},\\frac{\\partial z}{\\partial u})$$$$B = N \\times T$$$$T = (\\frac{\\partial x}{\\partial u},\\frac{\\partial y}{\\partial u},\\frac{\\partial z}{\\partial u}) \\times (\\frac{\\partial x}{\\partial v},\\frac{\\partial y}{\\partial v},\\frac{\\partial z}{\\partial v}) $$ 这就是法线贴图中存储的值。 结语下一次的shader我们将来写 半透明的边缘发光 效果。为此在下一篇我们将会先梳理介绍一下Unity shader透明效果的知识。","link":"/2019/05/02/shader-learning4/"},{"title":"Unity Shader 入门（八）：非真实感渲染知识准备","text":"前言非真实感渲染（Non-Photorealistic Rendering，NPR）， 亦被称为风格化渲染（Stylistic Rendering），与传统的追求照片真实感的真实感渲染（Photorealistic Rendering）计算机图形学不同，非真实感渲染旨在模拟艺术式的绘制风格，使用一些渲染方法达到和某些绘画风格相似的效果。NPR 的目的之一就是创建类似技术示意图、技术图纸相关的图像，而另一个应用领域便是对绘画风格和自然媒体（如铅笔、钢笔、墨水、木炭、水彩画等）进行模拟。这是一个涉及内容非常之多的应用领域，为了捕捉各种媒体的真实效果，人们已经提出了各种不同的算法。 非真实感渲染与我们并不遥远，它早以“卡通着色（Toon Shading）”的形式出现在各式动漫和电影中。在游戏制作方面，各种涉及到非真实感渲染的作品数不胜数，《Ōkami(大神)》，《The Legend of Zelda（塞尔达传说）》系列，甚至到现在的《Dota2》、《英雄联盟》、《守望先锋》，都多多少少涉及到了 NPR。 卡通渲染上文提到，一直以来，有一种特殊形式的 NPR 倍受关注，且和我们的生活息息相关，那就是卡通渲染（Toon Rendering，又称 Cel Rendering）。这种渲染风格能够给人以独特的感染力与童趣。这种风格很受欢迎的原因之一是 McCloud 的经典著作《Understanding Comics》中所讲述到的“通过简化进行增强（Amplification Through Simplification）”。通过简化并剔除所包含的混杂部分，可以突出于主题相关的信息，而大部分观众都会认同那些用简单风格描绘出来的卡通形象。 在计算机图形学领域，大约在 20 世纪 90 年代就开始使用 toon 渲染风格来实现三维模型和二维的传统动画之间的结合。而且和其他 NPR 风格相比，这种绘制方法比较简单，可以很容易地利用计算机进行自动生成。可以将最卡通着色基本的三个要素概括为： 锐利的阴影（Sharp shadows） 少有或没有高亮的点（Little or no highlight） 对物体轮廓进行描边（Outline around objects） 渲染方法关于 toon 渲染，有很多不同的实现方法。 对于含有纹理但没有光照的模型来说，可以通过对纹理进行量化来近似具有实心填充颜色的卡通风格。 对于明暗处理，有两种最为常见的方法，一种是用实心颜色填充多边形区域。但这种方式实用价值不大。另一种是使用 2-tone 方法来表示光照效果和阴影区域。也称为硬着色方法（Hard Shading），可以通过将传统光照方程元素重新映射到不同的调色板上来实现。此外，一般用黑色来绘制图形的轮廓，可以达到增强卡通视觉效果的目的。 具体的着色方法，可以理解为在 Fragment shader 中测试每个像素漫反射 diffuse 中的 NdotL 值，让漫反射形成一个阶梯函数，不同的 NdotL 区域对应不同的颜色。下图显示了不同的漫反射强度值的着色部分阶梯指定了不同的像素颜色。 轮廓描边我们在上一节写的模型描边是卡通渲染中一个非常常用的技术，完成描边的算法可以分为以下五种： 基于视点方向的描边 基于过程几何方法的描边 基于图像处理的描边 基于轮廓边缘检测的描边 混和轮廓描边 基于视点方向的描边基于视点方向的描边方法，即表面角描边（Surface Angle Silhouetting），其基本思想是使用视点方向（view point）和表面法线（surface normal）之间的点乘结果得到轮廓线信息。如果此点乘结果接近于零，那么可以断定这个表面极大概率是侧向（Edge-on）的视线方向，而我们就将其视做轮廓边缘，进行描边。 基于过程几何方法的描边基于过程几何方法生成的描边，即过程几何描边（Procedural Geometry Silhouetting），基本思想是先渲染正向表面（frontfaces），再渲染背向表面（backfaces），从而使得轮廓边缘可见，达到描边的目的。有多种方法用来渲染背向表面，且各有优缺点。但它们都是先渲染正向表面，然后打开正向表面裁剪（culling）开关，同时关闭背向裁剪开关。这样这个 pass 中的渲染结果便只会显示出背向表面。一种基于过程几何方法生成的描边的方法是仅仅渲染出背向表面的边界线（而不是面），使用偏置（Biasing）或者其他技术来确保这些线条恰好位于正向表面之前。这样就可以将除轮廓边缘之外的其他所有线条全部隐藏起来。这种方法非常适合单像素宽的线条，但如果线条的宽度超过这个值，那么通常会出现无法连接独立线段的情况，从而造成明显的缝隙。另一种渲染较宽描边线条的方法是直接将背面表面本身渲染成黑色。但没有任何偏置操作，背向表面就会保持不可见，所以需要做的就是通过偏置将这些背向表面沿屏幕 Z 方向向前移动，这样，便只有背向表面的三角形边缘是可见的。如下图，可以使用背向表面的斜率对对多边形进行向前偏置，但是线条宽度依然依赖于正向表面的角度。 基于图像处理的描边基于图像处理生成轮廓描边（Silhouetting by Image Processing），即通过在各种缓冲区上执行图像处理技术，来实现非真实渲染的方法。可以将其理解为一种后处理操作。通过寻找相邻 Z 缓冲数值的不连续性，就可以确定大多数轮廓线的位置。同样，借助邻接表面法线向量的不连续性，可以确定出分界线（往往也是轮廓线）边缘的位置。此外，利用环境色对场景进行绘制，也可以用来检测前两种方法可能会漏掉的边缘。 基于轮廓边缘检测的描边上文提到的大多数渲染描边的方法都存在一个缺点，那就是他们都需要两个通道才能完成物体轮廓描边的渲染。基于轮廓边缘检测的描边，通过检测出轮廓边缘（Silhouette Edge Detection）），并直接对它们进行绘制，这种形式的描边，可以很好地控制线条绘制的过程。由于边缘独立于模型，因此这种方法还有另外一个优点就是能够生成一些特殊的效果。例如，在网格密集的地方可以突现出轮廓边缘。可以将轮廓边缘理解为朝向相反的相邻三角形的交接。也就是说，其中的一个三角形是朝向视点，另一个三角形背向视点。具体测试方法如下：$$( n_0 · v &gt; 0 ) \\ne ( n_1 · v &gt; 0 )$$其中 $n_0$ 和 $n_1$ 分别表示两个三角形的表面法线向量，v 表示从视点到这条边缘（也就是其中任何一个端点）的视线方向向量。而为了确保这种测试的准确性，必须保证表面的取向一致。 混和轮廓描边混和轮廓描边（Hybrid Silhouetting），即结合了图像处理方法和几何要素方法，来渲染轮廓的方法。这种方法的具体思想是： 找到一系列轮廓边缘的列表。其次，渲染出所有物体的三角形和轮廓边缘，同时为他们指定一个不同的 ID 值（也就是说，赋予不同的颜色）。 读取该 ID 缓冲器并从中判断出可见的轮廓边缘，随之对这些可见线段进行重叠检测，并将它们连接起来形成平滑的笔划路径。 最后就可以对这些重建起来的路径进行风格化笔划渲染，其中，这些笔划本身可以用很多方法来进行风格化处理，包括变细、火焰、摆动、淡化等效果，同时还有深度和距离信息。如下图。 纹理调色板（ Palette of Textures ）除了 toon 渲染这种比较受欢迎的模拟风格之外，还存在其他各式各样的风格。NPR 效果涵盖的范围非常广泛，例如其中的一种：纹理调色板（palette of textures）由 Lake 等人讨论提出，基本思想是通过反射着色项（diffuse shading term）的不同，来选择应用于物体表面上的不同纹理。随着漫反射项逐渐变暗，可以用相应更暗的纹理，而为了能够产生手绘的效果，可以使用屏幕空间坐标来采样纹理。同时，为了增强绘制效果，可以在屏幕空间上的所有表面上运用纸纹理。随着物体的运动，他们就可以在纹理之间进行穿梭。原因在于这个纹理是在屏幕空间中实现的。此外，也可以在世界空间中运用这个纹理，这样就能够得到一个与屏幕空间完全不同的效果。 色调艺术图（ Tonal Art Maps/TAM ）通过在纹理之间进行切换形成的硬着色效果和 toon 着色效果之间的一种混合，Praun 等人以实时生成笔划纹理分级细化图的方法，并可以将其以平滑的方式运用到物体表面上。第一步是生成即时使用的纹理，称为色调艺术图（Tonal Art Maps，TAM）,主要思想是将笔划绘制为分级细分图层次，如图。 嫁接（ Graftals ）嫁接（Graftals）的基本思想，是将几何或者贴花纹理应用到物体表面，从而产生某种特殊效果。可以通过所需要的细节层次，物体表面相对视点的方位或者其他因素，对纹理进行控制。这种方法可以用来模拟钢笔或者画刷的笔刷，如下图。","link":"/2019/05/18/shader-learning8/"},{"title":"Unity Shader 入门（九）：卡通高光shader","text":"前言前一篇介绍了非真实渲染的理论知识，现在来写一个卡通高光效果，效果图如下： 照例我们还是先放上完整的代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182Shader \"Kurong/NPR/ToonShaer\"{ Properties { _Color (\"Color\", Color) = (1, 1, 1, 1) [HDR]_AmbientColor(\"Ambient Color\", Color) = (1,1,1,1) _MainTex(\"Main Texture\", 2D) = \"white\" {} [HDR]_SpecularColor(\"Specular Color\", Color) = (1,1,1,1) _Glossiness(\"Glossiness\", Float) = 32 [HDR]_RimColor(\"Rim Color\", Color) = (1,1,1,1) _RimAmount(\"Rim Amount\", Range(0, 1)) = 0.716 _RimThreshold(\"Rim Threshold\", Range(0, 1)) = 0.1 } SubShader { Tags { \"LightMode\" = \"ForwardBase\" \"PassFlags\" = \"OnlyDirectional\" } LOD 200 Pass { CGPROGRAM #pragma vertex vert #pragma fragment frag #include \"UnityCG.cginc\" #include \"Lighting.cginc\" sampler2D _MainTex; float4 _MainTex_ST; float4 _Color; float4 _AmbientColor; float _Glossiness; float4 _SpecularColor; float4 _RimColor; float _RimAmount; float _RimThreshold; struct a2f { float4 vertex : POSITION; float4 uv : TEXCOORD0; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float2 uv : TEXCOORD0; float3 worldNormal : NORMAL; float3 viewDir : TEXCOORD1; }; v2f vert (a2f v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.uv = TRANSFORM_TEX(v.uv,_MainTex); o.worldNormal = UnityObjectToWorldNormal(v.normal); o.viewDir = WorldSpaceViewDir(v.vertex); return o; } float4 frag (v2f v) : SV_Target { float3 normal = normalize(v.worldNormal); float dotL = dot(normal,_WorldSpaceLightPos0); float lightIntensity = smoothstep(0, 0.01, dotL);; float4 light = lightIntensity * _LightColor0; float3 viewDir = normalize(v.viewDir); float3 halfVector = normalize(_WorldSpaceLightPos0 + viewDir); float dotH = dot(normal, halfVector); float specularIntensity = pow(dotH * lightIntensity, _Glossiness * _Glossiness); float specularIntensitySmooth = smoothstep(0.005, 0.01, specularIntensity); float4 specular = specularIntensitySmooth * _SpecularColor; float4 rimDot = 1 - dot(viewDir, normal); float rimIntensity = smoothstep(_RimAmount - 0.01, _RimAmount + 0.01, rimDot); float4 rim = rimIntensity * _RimColor; float4 sample = tex2D(_MainTex, v.uv); return _Color * (_AmbientColor + light + specular + rim); } ENDCG } } FallBack \"Diffuse\"} 前期准备首先我们准备一个非常简单的Shader，去掉默认的光照，直接输出一个单一的颜色 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849Shader \"Kurong/NPR/MainColor\"{ Properties { _Color (\"Color\", Color) = (1,1,1,1) _MainTex (\"Albedo (RGB)\", 2D) = \"white\" {} } SubShader { Pass { Tags { \"LightMode\"=\"ForwardBase\" } LOD 200 CGPROGRAM #pragma vertex vert #pragma fragment frag #include \"UnityCG.cginc\" #include \"Lighting.cginc\" sampler2D _MainTex; float4 _Color; float4 _MainTex_ST; struct a2f { float4 vertex : POSITION; }; struct v2f { float4 pos : SV_POSITION; }; v2f vert (a2f v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); return o; } float4 frag (v2f v) : SV_Target { return _Color; } ENDCG } } FallBack \"Diffuse\"} 单一光源我们之前常用的表面着色器里面包含了光照效果，例如我们在查看第一个Shader的时候看见： 1#pragma surface surf Standard fullforwardshadows 但有的时候我们希望这个材质只受单一的光源影响（在卡通渲染中经常用到），因此在Shader代码中我们进行设置： 1Tags { \"LightMode\" = \"ForwardBase\" \"PassFlags\" = \"OnlyDirectional\" } 更多的tag可以查看传送门，这里我们采用的光照模型是 Blinn-Phong Blinn-Phong 光照模型 可以看到共有的条件有： 光源方向 L 法线方向 N 视线方向 V 反射方向 R 而 Blinn-Phong 模型引入一个新的矢量 H ，通过视角方向和光源方向相加得到，然后使用 $\\vec N$ 和 $\\vec H$ 的夹角进行计算： $$ H = \\frac{\\vec V + \\vec L}{|\\vec V+\\vec L|}$$ 总结一下Blinn模型的公式是： $$ C_{specular} = (C_{light} · M_{specular})max(0,\\vec N · \\vec H)$$ 这里我们加入法线信息： 1234567891011121314151617181920212223242526struct a2f{ float4 vertex : POSITION; float3 normal : NORMAL;};struct v2f{ float4 pos : SV_POSITION; float3 worldNormal : NORMAL;};v2f vert (a2f v){ v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.worldNormal = UnityObjectToWorldNormal(v.normal); return o;}float4 frag (v2f v) : SV_Target{ float3 normal = normalize(v.worldNormal); float NdotL = dot(_WorldSpaceLightPos0 , normal); return _Color * NdotL;} 注意 _WorldSpaceLightPos0 表示的是当前光源的位置。这样看起来是比较真实感的渲染，卡通渲染的一大特点就是明暗交界线十分明显，因此我们做一点小小的调整： 1234567float4 frag (v2f v) : SV_Target{ float3 normal = normalize(v.worldNormal); float NdotL = dot(_WorldSpaceLightPos0 , normal); float lightIntensity = NdotL &gt; 0 ? 1 : 0; return _Color * lightIntensity;} 添加反射光接下来我们添加反射光的颜色，添加属性： 1[HDR]_AmbientColor(\"Ambient Color\", Color) = (1,1,1,1) 12345678float4 frag (v2f v) : SV_Target{ float3 normal = normalize(v.worldNormal); float NdotL = dot(_WorldSpaceLightPos0 , normal); float lightIntensity = NdotL &gt; 0 ? 1 : 0; float4 light = lightIntensity * _LightColor0; return _Color * (light + _AmbientColor);} 添加 light 用来收集和光照有关的数据，_LightColor0 表示单一光源的颜色。这里我们注意到明暗交界的地方锯齿比较严重，修改一下代码 1float lightIntensity = smoothstep(0, 0.01, dotL); 添加高光卡通风格的高光往往是模型上一块明显的光斑，根据上面提到 Blinn-Phong 公式我们添加代码： 12[HDR]_SpecularColor(\"Specular Color\", Color) = (1,1,1,1)_Glossiness(\"Glossiness\", Float) = 32 123456struct v2f{ float4 pos : SV_POSITION; float3 worldNormal : NORMAL; float3 viewDir : TEXCOORD1;}; 123456789101112131415161718192021v2f vert (a2f v){ v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.worldNormal = UnityObjectToWorldNormal(v.normal); o.viewDir = WorldSpaceViewDir(v.vertex); return o;}float4 frag (v2f v) : SV_Target{ float3 normal = normalize(v.worldNormal); float NdotL = dot(_WorldSpaceLightPos0 , normal); float lightIntensity = smoothstep(0, 0.01, NdotL); float4 light = lightIntensity * _LightColor0; float3 viewDir = normalize(v.viewDir); float3 halfVector = normalize(_WorldSpaceLightPos0 + viewDir); float NdotH = dot(normal, halfVector); float specularIntensity = pow(NdotH * lightIntensity, _Glossiness * _Glossiness) * _SpecularColor; return _Color * (light + _AmbientColor + specularIntensity);} 和明暗边界线的解决方案一样，也做一些小小的更改： 123float specularIntensitySmooth = smoothstep(0.005, 0.01, specularIntensity);float4 specular = specularIntensitySmooth * _SpecularColor;return _Color * (light + _AmbientColor + specular); 添加描边我们在发光的地方添加描边，模型更加卡通化： 123[HDR]_RimColor(\"Rim Color\", Color) = (1,1,1,1)_RimAmount(\"Rim Amount\", Range(0, 1)) = 0.5_RimThreshold(\"Rim Threshold\", Range(0, 1)) = 0.1 12345float4 rimDot = 1 - dot(viewDir, normal);float rimIntensity = rimDot * pow(NdotL, _RimThreshold);;rimIntensity = smoothstep(_RimAmount - 0.01, _RimAmount + 0.01, rimIntensity);float4 rim = rimIntensity * _RimColor;return _Color * (light + _AmbientColor + specular + rim); 添加阴影最后一步添加阴影（场景中添加一个地板和遮蔽的物体）： 12345678910111213#pragma multi_compile_fwdbase#include \"AutoLight.cginc\"struct v2f{ float4 pos : SV_POSITION; float3 worldNormal : NORMAL; float3 viewDir : TEXCOORD1; SHADOW_COORDS(2)};float shadow = SHADOW_ATTENUATION(v);float lightIntensity = smoothstep(0, 0.01, NdotL * shadow);","link":"/2019/05/23/shader-learning9/"},{"title":"Unity Shader 入门（一）：渲染管线","text":"什么是渲染管线？Shader（着色器）：是渲染管线上的一小段程序，要了解Shader首先要明白渲染管线是什么呢？ 渲染管线的主要功能是在给定虚拟相机、三维物体、光源、照明模式，以及纹理等诸多条件的情况下，生成或绘制一幅二维图像的过程。对于实时渲染来说，渲染管线就是基础，可以说渲染管线是实时渲染的底层工具。 在概念上可以将图形渲染管线分为四个阶段： 应用程序阶段（The Application） 几何阶段（The Geometry） 光栅化阶段（The Rasterizer） 像素处理阶段（pixel processing） 这里有几个要点： 每个阶段本身也可能是一条管线，如图中的几何阶段所示。此外，还可以对有的阶段进行全部或者部分的并行化处理，如图中的像素处理阶段。应用程序阶段虽然是一个单独的过程，但是依然可以对之进行管线化或者并行化处理。 最慢的管线阶段决定绘制速度，即图像的更新速度，这种速度一般用 frames per second(FPS) 来表示，也就是每秒绘制的图像数量，或者用 Hertz (Hz) 来表示。 应用程序阶段 The Application Stage顾名思义，应用程序阶段是由应用程序驱动的，一般是图形渲染管线概念上的第一个阶段。开发者能够对该阶段发生的情况进行完全控制，可以通过改变实现方法来改变实际性能，可以做的任务包括： 碰撞检测、输入检测、力反馈 纹理动画、变换仿真、几何形变 等等 在其他阶段是全部或者部分建立在硬件基础上，因此要改变实现过程会非常困难。 正因应用程序阶段是软件方式实现，因此不能像几何和光栅化阶段那样继续分为若干个子阶段。但为了提高性能，该阶段还是可以在几个并行处理器上同时执行。在 CPU 设计上，称这种形式为超标量体系（superscalar）结构，因为它可以在同一阶段同一时间做不同的几件事情。 应用程序阶段通常完成一些不在其他阶段执行的计算，如层次视锥裁剪等加速算法就可以在这里实现。 应用程序阶段的主要任务：在应用程序阶段的末端，将需要在屏幕上（具体形式取决于具体输入设备）显示出来绘制的几何体（也就是绘制图元(rendering primitives)：如点、线、矩形等）、以及摄像机位置输入到绘制管线的下一个阶段————几何阶段。 几何阶段 The Geometry Stage这个阶段用于处理几乎所有我们要绘制的几何相关的事情，比如决定画什么、怎么画、画在哪里等等（这一段主要在GPU上进行），因为事情太多，因此可以进一步细分成一个小的流水线： 顶点着色 Vertex Shading 投影 Projection 裁剪 Clipping 屏幕映射 Screen Mapping 顶点着色 Vertex Shading顶点着色 Vertex Shading 阶段主要的任务是： 计算顶点的位置 计算程序员要的顶点的输出数据例如：法线、纹理贴图坐标等等 一般来说, 模型大部分的着色是通过：计算光源作用于每个顶点的位置、法线信息的结果，存储时仅存储顶点处的颜色，然后在三角形上插值这些颜色来得到的。顶点着色器现在是一个更通用的部分, 专门用于设置与每个顶点关联的数据比如可以用于顶点绑定、图像变换等等。 在绘制到屏幕上之前，模型通常需要变换到若干不同的空间或坐标系中。模型变换的变换对象一般是模型的顶点和法线。物体的坐标称为模型坐标。世界空间是唯一的，所有的模型经过变换后都位于同一个空间中。 就像上文提到，应该仅对相机（或者视点）可以看到的模型进行绘制，而相机在世界空间中有一个位置和一个方向。为了方便投影和裁剪，必须对相机和所有的模型进行视点变换。变换的目的就是要把相机放在原点，然后进行视点校准，使其朝向 Z 轴负方向，y 轴指向上方,x 轴指向右边。在视点变换后，实际位置和方向就依赖于当前的 API。我们称上述空间为相机空间或者观察空间，下图显示了视点变换对相机和模型的影响。 为了产生逼真的场景，仅仅渲染形状和位置是远远不够的，我们需要对物体的外观进行建模。而物体经过建模，会得到对包括每个对象的材质，以及照射在对象上的任何光源的效果在内的一些刻画。且光照和材质可以用任意数量的方式，从简单的颜色描述到复杂的物理描述来模拟。 确定材质上的光照效果的这种操作被称为着色（shading），着色过程涉及在对象上的各个点处计算着色方程（shading equation）。通常，这些计算中的一些在几何阶段期间在模型的顶点上执行（vertex shading），而其他计算可以在每像素光栅化（per-pixel rasterization）期间执行。可以在每个顶点处存储各种材料数据，诸如点的位置，法线，颜色或计算着色方程所需的任何其它数字信息。顶点着色的结果（其可以是颜色，向量，纹理坐标或任何其他种类的着色数据）计算完成后，会被发送到光栅化阶段以进行插值操作。 着色计算通常认为是在世界空间中进行的。在实践中，有时需要将相关实体（诸如相机和光源）转换到一些其它空间（诸如模型或观察空间）并在那里执行计算，也可以得到正确的结果。这是因为如果着色过程中所有的实体变换到了相同的空间，着色计算中需要的诸如光源，相机和模型之间的相对关系是不会变的。 投影 Projection在光照处理之后，渲染系统就开始进行投影操作，即将视体变换到一个对角顶点分别是 (-1,-1,-1) 和 (1,1,1) 单位立方体（unit cube）内，这个单位立方体通常也被称为规范视域体（Canonical View Volume，CVV）。目前，主要有两种投影方法，即： 正交投影（orthographic projection，或称 parallel projection）：正交投影的可视体通常是一个矩形，正交投影可以把这个视体变换为单位立方体。正交投影的主要特性是平行线在变换之后彼此之间仍然保持平行，这种变换是平移与缩放的组合。 透视投影（perspective projection）：相比之下，透视投影比正交投影复杂一些。在这种投影中，越远离摄像机的物体，它在投影后看起来越小。更进一步来说，平行线将在地平线处会聚。透视投影的变换其实就是模拟人类感知物体的方式。 正交投影和透视投影都可以通过 4 x 4 的矩阵来实现，在任何一种变换之后，都可以认为模型位于归一化处理之后的设备坐标系中。虽然这些矩阵变换是从一个可视体变换到另一个，但它们仍被称为投影，因为在完成显示后，Z 坐标将不会再保存于的得到的投影图片中。通过这样的投影方法，就将模型从三维空间投影到了二维的空间中。 可选择的顶点处理这一部分是这个流水线上的可选项，不一定必须经历这个环节，是否启用取决于硬件的条件，这些环节彼此之间是相互独立的，按照顺序是： 曲面细分 tessellation：例如我们用三角形来描绘物体，理论上将三角形划分的越细物体也就越精细，如果这个物体离摄像头很远或者只在摄像头内出现一角，花费大量的三角形细分就比较浪费资源了。但在应用曲面细分，曲面就可以用适当的三角形数来生成。 几何元着色 geometry shading：这个阶段的作用和曲面细分类似，都可以生成新的顶点，但这是一个输出与输入的图元都受限的阶段，相应的也较为简单快速。最广泛应用这个阶段的地方可能是 生成粒子 例如烟火效果，每一点火星就好像一个点，几何元着色可以将这个点由一个面对观众的正方形（两个三角形组成）来表示，这更方便我们渲染图元。 流式输出 stream output：这个阶段让我将GPU当作几何引擎，这个阶段我们不是把数据传给剩余的流水线去处理，而是可以选择将它们输出到数组以进行进一步处理，这些数据可以给CPU，或者GPU自己，这个阶段在经常用于生成粒子，例如上面烟火的例子。 裁剪 Clipping只有当图元完全或部分存在于视体（也就是上文的规范视域体CVV）内部的时候，才需要将其发送到光栅化阶段，这个阶段可以把这些图元在屏幕上绘制出来。显然一个图元相对视体内部的位置，分为三种情况：完全位于内部、完全位于外部、部分位于内部。所以就要分情况进行处理： 当图元完全位于视体内部：那么它可以直接进行下一个阶段。 当图元完全位于视体外部：不会进入下一个阶段，可直接丢弃，因为它们无需进行渲染。 当图元部分位于视体内部：则需要对那些部分位于视体内的图元进行裁剪处理。对部分位于视体内部的图元进行裁剪操作，这就是裁剪过程存在的意义。裁剪过程见下图。 屏幕映射 Screen Mapping只有在视体内部经过裁剪的图元，以及之前完全位于视体内部的图元，才可以进入到屏幕映射阶段。进入到这个阶段时，坐标仍然是三维的（但显示状态在经过投影阶段后已经成了二维），每个图元的 x 和 y 坐标变换到了屏幕坐标系中，屏幕坐标系连同 z 坐标一起称为窗口坐标系。假定在一个窗口里对场景进行绘制，窗口的最小坐标为（x1，y1），最大坐标为（x2，y2），其中 x1 &lt; x2，y1 &lt; y2。屏幕映射首先进行平移，随后进行缩放，在映射过程中 z 坐标不受影响。新的 x 和 y 坐标称为屏幕坐标系，与 z 坐标一起（OpenGL下是[-1,+1]，DirectX下是[0,1]）进入光栅化阶段。如下图： 屏幕映射阶段的一个常见困惑是整型和浮点型的点值如何与像素坐标（或纹理坐标）进行关联。因为像素的中点可以定义在0.5处，因此索引在[0,9]之内的像素可以表示的浮点范围在[0.0,10.0)之内，这个转换可以简单的通过以下来转变：$$d=floor(c)$$$$c=d+0.5$$其中 d 是像素的整数索引, c 是像素内的浮点值。 光栅化阶段 The Rasterizer Stage给定经过变换和投影之后的顶点，颜色以及纹理坐标（均来自于几何阶段），给每个像素（Pixel）正确配色，以便正确绘制整幅图像。这个过个过程叫光珊化（rasterization）这个阶段主要分成两部分：三角形设定（Triangle Setup）也被称为图元装配和三角形遍历（Triangle Traversal），即从二维顶点所处的屏幕空间（所有顶点都包含 Z 值即深度值，及各种与相关的着色信息）到屏幕上的像素的转换。 三角形设定 Triangle Setup三角形设定阶段主要用来计算三角形表面差异和边缘方程等其他相关数据。该数据主要用于扫描转换（scan conversion），以及由几何阶段处理的各种着色数据的插值操作所用，该过程在专门为其设计的硬件上执行。 三角形遍历 Triangle Traversal在三角形遍历阶段将进行逐像素检查操作，检查该像素处的像素中心是否由三角形覆盖，而对于有三角形部分重合的像素，将在其重合部分生成片段（fragment）。找到哪些采样点或像素在三角形中的过程通常叫三角形遍历（TriangleTraversal）或扫描转换（scan conversion）。每个三角形片段的属性均由三个三角形顶点的数据插值而生成。这些属性包括片段的深度，以及来自几何阶段的着色数据。 像素处理 Pixel Processing在这个阶段，所有的像素已经都在一个三角形中或者是前面一系列处理之后的图元下，像素处理阶段被分为像素着色和融合两个子阶段 像素着色 Pixel Shading所有逐像素的着色计算都在像素着色阶段进行，使用插值得来的着色数据作为输入，输出结果为一种或多种将被传送到下一阶段的颜色信息。纹理贴图操作就是在这阶段进行的。像素着色阶段是在可编程 GPU 内执行的，在这一阶段有大量的技术可以使用，其中最常见，最重要的技术之一就是纹理贴图（Texturing）。纹理贴图在书的第六章会详细讲到。简单来说，纹理贴图就是将指定图片“贴”到指定物体上的过程。而指定的图片可以是一维，二维，或者三维的，其中，自然是二维图片最为常见。如下图所示： 融合 Merging每个像素的信息都储存在颜色缓冲器中，而颜色缓冲器是一个颜色的矩阵列（每种颜色包含红、绿、蓝三个分量）。融合阶段的主要任务是合成当前储存于缓冲器中的由之前的像素着色阶段产生的片段颜色。不像其它着色阶段，通常运行该阶段的 GPU 子单元并非完全可编程的，但其高度可配置，可支持多种特效。 这个阶段还负责可见性问题的处理。这意味着当绘制完整场景的时候，颜色缓冲器中应该还包含从相机视点处可以观察到的场景图元。对于大多数图形硬件来说，这个过程是通过 Z 缓冲（也称深度缓冲器）算法来实现的。Z 缓冲算法非常简单，具有 $O(n)$ 复杂度（n 是需要绘制的像素数量），只要对每个图元计算出相应的像素 z 值，就可以使用这种方法，大概内容是： Z 缓冲器和颜色缓冲器形状大小一样，每个像素都存储着一个 z 值，这个 z 值是从相机到最近图元之间的距离。 每次将一个图元绘制为相应像素时，需要计算像素位置处图元的 z 值，并与同一像素处的 z 缓冲器内容进行比较。 如果新计算出的 z 值，远远小于 Z 缓冲器中的 z 值，那么说明即将绘制的图元与相机的距离比原来距离相机最近的图元还要近。这样像素的 z 值和颜色就由当前图元对应的值和颜色进行更新。反之，若计算出的 z 值远远大于 z 缓冲器中的 z 值，那么 z 缓冲器和颜色缓冲器中的值就无需改变。 值得注意的是这个算法不能用于渲染半透明物体，原因我们会在往后的文章解释 颜色缓冲器用来存储颜色，z 缓冲器用来存储每个像素的 z 值，还有其他缓冲器可以用来过滤和捕获片段信息。 比如 alpha 通道（alpha channel）和颜色缓冲器联系在一起可以存储一个与每个像素相关的不透明值。可选的 alpha 测试可在深度测试执行前在传入片段上运行。片段的 alpha 值与参考值作某些特定的测试（如等于，大于等），如果片断未能通过测试，它将不再进行进一步的处理。alpha 测试经常用于不影响深度缓存的全透明片段的处理。 模板缓冲器（stencil buffer）是用于记录所呈现图元位置的离屏缓存。每个像素通常与占用 8 个位。图元可使用各种方法渲染到模板缓冲器中，而缓冲器中的内容可以控制颜色缓存和 Z 缓存的渲染。举个例子，假设在模版缓冲器中绘制出了一个实心圆形，那么可以使用一系列操作符来将后续的图元仅在圆形所出现的像素处绘制，类似一个 mask 的操作。模板缓冲器是制作特效的强大工具。而在管线末端的所有这些功能都叫做光栅操作（raster operations ，ROP）或混合操作（blend operations）可以将颜色缓冲区中当前的颜色与在三角形内处理的像素的颜色混合，这样可以实现半透明或者颜色累积的效果。 帧缓冲器（frame buffer）通常包含一个系统所具有的所有缓冲器，但有时也可以认为是颜色缓冲器和 Z 缓冲器的组合。 而当图元通过光栅化阶段之后，从相机视点处看到的东西就可以在荧幕上显示出来。为了避免观察者体验到对图元进行处理并发送到屏幕的过程，图形系统一般使用了双缓冲（double buffering）机制，这意味着屏幕绘制是在一个后置缓冲器（back buffer）中以离屏的方式进行的。一旦屏幕已在后置缓冲器中绘制，后置缓冲器中的内容就不断与已经在屏幕上显示过的前置缓冲器中的内容进行交换，当然，只有当不影响显示的时候才进行交换。","link":"/2019/04/28/shader-learning1/"}],"tags":[{"name":"Shader、图形学","slug":"Shader、图形学","link":"/tags/Shader、图形学/"}],"categories":[{"name":"Shader","slug":"Shader","link":"/categories/Shader/"}]}