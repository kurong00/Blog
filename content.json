{"pages":[],"posts":[{"title":"Unity Shader 入门（三）：编写第一个Shader","text":"编写第一个Shader上一节我们学习了第一个简单的Shader，现在我们可以开始写第一个shader练练手了（搓搓手）。首先我们挑一个【边缘发光（水晶球）】的shader来写 首先来看一下效果图，如果你感兴趣的话就接下来看吧： 实现原理：根据物体表面法向量和视线向量的夹角来判断是否是物体的边缘部位。夹角越大（接近垂直）说明越接近物体边缘部分。重点：向量点积运算。 具体解说：先放一段实现的代码： Shader \"Custom/Rim//RimBump\" { Properties{ _Color(\"Main Color\", Color) = (1,1,1,1) _SpecColor(\"Specular Color\", Color) = (0.5, 0.5, 0.5, 1) _BumpMap(\"Normalmap\", 2D) = \"bump\" {} _RimColor(\"Rim Color\", Color) = (0.26,0.19,0.16,0.0) _RimPower(\"Rim Power\", Range(0.5,8.0)) = 2.0 } SubShader{ Tags { \"RenderType\" = \"Opaque\" } LOD 400 CGPROGRAM #pragma surface surf BlinnPhong #pragma target 3.0 sampler2D _BumpMap; fixed4 _Color; float4 _RimColor; float _RimPower; struct Input { float2 uv_MainTex; float2 uv_BumpMap; float3 viewDir; }; void surf(Input IN, inout SurfaceOutput o) { o.Albedo = _Color.rgb; o.Gloss = 1; o.Normal = UnpackNormal(tex2D(_BumpMap, IN.uv_BumpMap)); half rim = 1 - saturate(dot(normalize(IN.viewDir), o.Normal)); o.Emission = _RimColor.rgb * pow(rim, _RimPower); } ENDCG } FallBack \"Diffuse\" } 如果你看过上一篇的Shader介绍你应该可以大致看懂上面的代码，我们就关键部分说明一下： void surf(Input IN, inout SurfaceOutput o) { o.Albedo = _Color.rgb; o.Gloss = 1; o.Normal = UnpackNormal(tex2D(_BumpMap, IN.uv_BumpMap)); half rim = 1 - saturate(dot(normalize(IN.viewDir), o.Normal)); o.Emission = _RimColor.rgb * pow(rim, _RimPower); } 首先这两句： o.Albedo = _Color.rgb; o.Gloss = 1; 类比上一篇，o.Albedo 此时可以获得我们设置的颜色和贴图之间混合后的颜色，o.Gloss 我们将发光强度设置成1。 接下来是重点：o.Normal = UnpackNormal(tex2D(_BumpMap, IN.uv_BumpMap)); half rim = 1 - saturate(dot(normalize(IN.viewDir), o.Normal)); o.Emission = _RimColor.rgb * pow (rim, _RimStrength); UnpackNormal 是定义在UnityCG.cginc文件中的方法（这个文件中包含了一系列常用的CG变量以及方法，在Unity安装路径中可以找到），UnpackNormal接受一个fixed4的输入，并将其转换为所对应的法线值（fixed3）。在解包得到这个值之后，将其赋给输出的Normal，接下来我们就可以来使用Normal值啦。有关法线贴图 这一点归类于扩展阅读，如果你想知道UnpackNormal的原理可以继续查看，如果不的话就跳过这一段吧！假设你想知道原理，那首先思考一个问题为什么法线贴图看起来大多是蓝色的？ 实际上，我们通常见到的这种偏蓝色的法线纹理中，存储的是在Tangent Space中的顶点法线方向。那么，问题又来了，什么是Tangent Space。在Tangent Space中，坐标原点就是顶点的位置，其中z轴是该顶点本身的法线方向（N）。这样，另外两个坐标轴就是和该点相切的两条切线。这样的切线是有无数条，但模型一般会给定该顶点的一个tangent。（给定的过程可以见这个链接） 通常我们所见的法线纹理是基于原法线信息构建的坐标系来构建出来的。那种偏蓝色的法线纹理其实就是存储在每个顶点各自的Tangent Space中法线的扰动方向。也就是说，如果一个顶点的法线方向不变，那么在它的Tangent Space中，新的normal值就是z轴方向，也就是说值为(0, 0, 1)。但这并不是法线纹理中存储的最终值：因为一个向量每个维度的取值范围在(-1, 1)，在法线贴图中被压缩在颜色的范围[0,1]中，所以需要转换：颜色 = 0.5 * 法线 + 0.5; 线 = 2 * (颜色 - 0.5); 这样，之前的法线值(0, 0, 1)实际上对应了法线纹理中RGB的值为(0.5, 0.5, 1)，而这个颜色也就是法线纹理中那大片的蓝色。这些蓝色实际上说明顶点的大部分法线是和模型本身法线一样的，不需要改变。总结一下就是，法线纹理的RGB通道存储了在每个顶点各自的Tangent Space中的法线方向的映射值。 下一个问题：Unity编辑器中加入一张发现贴图，编辑器都会提示把法线纹理的“Texture Type”设置成“Normal Map”，这是为什么呢？是因为这样的设置可以让Unity根据不同平台对纹理进行压缩，当需要法线信息时，再通过UnpackNormal函数对法线纹理进行正确的采样，即将把颜色通道变成一个适合于实时法向映射的格式。 再下一个问题：压缩的内容又是什么呢？其实法线贴图只有两个通道是真正必不可少的，因为第三个通道的值可以用另外两个推导出来（法线是单位向量）法线（x,y,z）是一条单位向量。所以知道了x,y,z里的任意两个，剩下的那个就可以通过计算得出。所以我们就可以使用2个通道的图储存x,y,z里的两个值，将xyz里剩余的值省略，通过计算得出。而压缩后的法线贴图，大小只有原来的1/4左右，故可以使用更大或者更多的贴图来提升画面品质。 重点讲解回到刚刚打断的地方，下面两句： half rim = 1 - saturate(dot(normalize(IN.viewDir), o.Normal)); o.Emission = _RimColor.rgb * pow (rim, _RimStrength); 首先我们看normalize函数：为了对向量进行归一化处理（这里传入IN.viewDir指的是：WorldSpace View Direction，也就是当前坐标的视角方向）。 dot函数：返回传入的两个参数的点积，saturate函数：判断传入的参数是否在0-1之间，如果小于0，返回 0；如果大于 1，返回1； 接着第二句：_RimColor.rgb * pow (rim, _RimStrength)从_RimColor参数获取自发光颜色再和发光的强度混合，最终将颜色赋值给像素的Emission（发散颜色） 以上就是边缘发光效果的实现。 结语下一次的shader我们将来写【半透明】的边缘发光效果。为此在下一篇我们将会梳理一下Unity shader透明效果的知识储配","link":"/2017/11/13/Unity Shader 入门（三）：编写第一个Shader/"},{"title":"Unity Shader 入门（一）：理论准备","text":"1. 什么是Shader？shader（着色器）是GPU的渲染流水线上的一小段程序，它负责将输入的Mesh（网格）以指定的方式，和输入的贴图或者颜色等组合作用后输出。绘图单元可以依据这个输出来将图像绘制到屏幕上。 2. Shader的分类？shader大体上可以分为两类： 表面着色器（Surface Shader）：已经为你做了大部分的工作，只需要简单的编写就可以实现很多不错的效果。 片元着色器（Fragment Shader）：可以做的事情更多，相应的难度也会加大。可以在比较低的层级上进行更复杂（或者针对目标设备更高效）的开发。 3. 什么是渲染流水线？既然shader所在的阶段是渲染流水线上的一部分，那渲染流水线又是什么呢？ 首先GPU上的渲染流水线任务是：从一个三维场景出发，把这些信息最终转换成一张二维图像。我们可以将渲染流水线分成三个阶段：应用阶段-&gt;几何阶段-&gt;光栅化阶段。 我们逐一来看三个阶段： 3.1 应用阶段 这个阶段是完全由开发者主导的，主要工作是： 准备数据：例如相机位置、模型位置、光源位置等等 粗粒度的剔除工作：把不可见的物体删除出去 设置渲染状态：例如使用的材质、纹理、shader等等 输出：需要渲染的几何信息，也就是渲染图元（rendering primitives），渲染图元可以是点、线、面等等，渲染图元就交给下一个几何阶段 几何阶段 这个阶段用于处理几乎所有我们要绘制的几何相关的事情，比如决定画什么、怎么画、画在哪里等等（这一段主要在GPU上进行），因为事情太多，因此可以进一步细分成一个小的流水线： 此时到达Vertex Shader，shader会进行一些操作，例如：改变顶点位置，对顶点进行坐标变换(模型空间-&gt;世界空间-&gt;裁剪空间-&gt;屏幕空间)，贴图位置转换等等。 下一步开始图元装配：将一个个零散的顶点组装成一个个三角形。 下一步曲面细分环节（可选项，不一定经历这个环节，在Direct3D 11、OpenGL 4、OpenGL ES 3.2以上才支持）：将上一部的图元进行细分。 再下一步几何元着色器（也是可选项，不一定经历这个环节,在Direct3D 10、OpenGL 3.2、OpenGL ES 3.2以上支持）：增加顶点或者片元数 下一步裁剪：裁剪位于视锥外的片元 最后一步屏幕映射：输出屏幕空间的二维坐标和每个顶点的深度值，着色等信息。 光栅化阶段 将上一步得到的信息(深度值，着色，屏幕坐标等等)进行插值运算，确认哪些像素该被绘制在屏幕上。以上是渲染流水线的一个简单说明，真实的实现过程远比上面描述的复杂，但是好在Unity Shader已经封装了非常多的功能，下一节我们将开始分析第一个Unity Shader。","link":"/2019/04/30/Unity Shader 入门（一）：理论准备/"},{"title":"Unity Shader 入门（五）：边缘发光透明版","text":"导语之前我们写过一个边缘发光的Shader（传送门），这一次我们来写这个的升级版：透明物体的边缘发光。 效果图首先我们还是来看一下效果图： Shader代码Shader \"Custom/Rim/RimEnerge\" { Properties { _Color(\"Main Color\",Color) = (0.6,0.6,0.6,1) _AlphaRange(\"Alpha Range\",Range(0,1)) = 0 _RimColor(\"Rim Color\",Color) = (1,1,1,1) } SubShader { Tags{ \"Queue\"=\"Transparent\" \"IgnoreProjector\"=\"True\" \"RenderType\"=\"Transparent\" } ZWrite Off Blend SrcAlpha OneMinusSrcAlpha LOD 200 Pass { CGPROGRAM #pragma vertex vert #pragma fragment frag #include \"Lighting.cginc\" struct a2v { float4 vertex : POSITION; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float3 normalDir : TEXCOORD0; float3 worldPos : TEXCOORD1; }; fixed4 _Color; float _AlphaRange; fixed4 _RimColor; v2f vert( a2v v ) { v2f o; o.pos = UnityObjectToClipPos(v.vertex) ; o.normalDir = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld,v.vertex).xyz; return o; } fixed4 frag( v2f v ):COLOR { float3 normal = normalize(v.normalDir); float3 viewDir = normalize(_WorldSpaceCameraPos - v.worldPos); float normalDotViewDir = saturate(dot(normal,viewDir)); fixed3 diffuse = normalDotViewDir *_Color; return fixed4(diffuse + _RimColor ,(1 - normalDotViewDir) * (1 - _AlphaRange) + _AlphaRange); } ENDCG } } Fallback \"Diffuse\" } 透明度混合上一篇我们了解了透明度混合的原理以及一些透明度知识（传送门），而Unity中，为了进行透明度混合，我们需要用到【Blend】命令： 语法 描述 Blend Off 关闭混合（这是默认的状态） Blend SrcFactor DstFactor 开启混合，该片元产生的颜色SrcFactor. 已存在于屏幕的颜色 DstFactor，然后将两者叠加在一起存入颜色缓冲。 Blend SrcFactor DstFactor, SrcFactorA DstFactorA 原理同上，不过使用了不同的混合因子 BlendOp Op 不同于上面的颜色混合，而是使用Blend Operation（传送门）来对它们进行操作 BlendOp OpColor, OpAlpha 原理同上，不过采用不同的Blend Operation来操作Color和Alpha的通道 混合因子： 名称 描述 One 因子为1，表示让源颜色或者目标颜色通过 Zero 因子为0，用来删除源颜色或者目标颜色 SrcColor 因子为源颜色 SrcAlpha 因子为源颜色的透明度 DstColor 因子为目标颜色 DstAlpha 因子为目标颜色的透明度 OneMinusSrcColor 因子为 (1 - 源颜色) 的值 OneMinusSrcAlpha 因子为 (1 - 源颜色的透明度) 的值 OneMinusDstColor 因子为 (1 - 目标颜色) 的值 OneMinusDstAlpha 因子为 (1 - 目标颜色的透明度) 的值 此时我们再来看上面这一块代码： Tags{ \"Queue\"=\"Transparent\" \"IgnoreProjector\"=\"True\" \"RenderType\"=\"Transparent\" } ZWrite Off Blend SrcAlpha OneMinusSrcAlpha LOD 200 这里有一些新的知识：之前提过半透明物体的渲染序列要设置成&quot;Queue&quot;=&quot;Transparent&quot;,而&quot;RenderType&quot;=&quot;Transparent&quot;表示我们使用了透明度混合。通常一个半透明的Shader Tags都包含这三条： \"Queue\"=\"Transparent\" \"IgnoreProjector\"=\"True\" \"RenderType\"=\"Transparent\" 接下来是 ZWrite Off : 我们在上一篇介绍过为什么透明度混合需要关闭深度写入 最后是 Blend SrcAlpha OneMinusSrcAlpha : 这里我们将源颜色的混合因子设置成SrcAlpha，将目标颜色的混合因子设置成 OneMinusSrcAlpha 以得到半透明效果。 结构体定义 struct a2v { float4 vertex : POSITION; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float3 normalDir : TEXCOORD0; float3 worldPos : TEXCOORD1; }; a2v ：包含顶点着色器要的模型数据 float4 vertex : POSITION;这一句表示：用模型顶点的坐标填充vertex变量。 float3 normal : NORMAL; 这一句表示：用模型空间的法线方向向量填充normal变量 v2f ：用于顶点着色器和片元着色器之间传递信息 float4 pos : SV_POSITION;这一句表示：用裁剪空间的位置信息填充pos变量 float3 normalDir : TEXCOORD0;这一句表示：用模型的第一套纹理坐标填充normalDir变量 float3 worldPos : TEXCOORD1;这一句表示：用模型的第二套纹理坐标填充worldPos变量 顶点着色器 v2f vert( a2v v ) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.normalDir = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld,v.vertex).xyz; return o; } UnityObjectToClipPos(v.vertex)是Unity5.6之后的写法，之前是mul(UNITY_MATRIX_MVP,v.vertex) 这一句的意思是:将模型空间的顶点信息转换到裁剪空间中的位置信息，然后将信息存储在o.pos中。 UnityObjectToWorldNormal(v.normal)这一句的意思是:法线从模型空间变换到世界空间中并计算物体在世界空间中的法线坐标。 mul(unity_ObjectToWorld,v.vertex).xyz;这一句的意思是：将顶点从模型空间转换到世界空间的信息存储到worldPos变量中。 片元着色器 fixed4 frag( v2f v ):COLOR { float3 normal = normalize(v.normalDir); float3 viewDir = normalize(_WorldSpaceCameraPos - v.worldPos); float normalDotViewDir = saturate(dot(normal,viewDir)); fixed3 diffuse = normalDotViewDir *_Color; return fixed4(diffuse + _RimColor ,(1 - normalDotViewDir) * (1 - _AlphaRange) + _AlphaRange); } fixed4 frag( v2f v ):COLOR 我们注意到片元着色器的后面跟着:COLOR ：这是Unity提供的Cg/HLSL语义。语义可以告诉shader数据的来源以及数据的输出。 float3 viewDir = normalize(_WorldSpaceCameraPos - v.worldPos); 这里我们用对象在世界坐标系中的位置减去摄像机的世界空间位置，并进行逐顶点归一化，赋给视线的方向 float normalDotViewDir = saturate(dot(normal,viewDir)) 我们获得法线与视线的夹角 fixed3 diffuse = normalDotViewDir *_Color; 这里我们视线与法线的夹角和主颜色相乘。 return fixed4(diffuse + _RimColor ,(1 - normalDotViewDir) * (1 - _AlphaRange) + _AlphaRange); 最后将混合后的颜色输出。","link":"/2017/11/25/Unity Shader 入门（五）：边缘发光透明版/"},{"title":"Unity Shader 入门（四）：透明效果知识储备","text":"导语首先一个问题：如果场景中有非常多的物体，彼此之间有互相遮挡的情况，那么这些物体是按照什么样的渲染顺序进行渲染的呢？ 深度缓冲实际上，由于深度缓存（z-buffer）的存在,不透明的物体在不考虑渲染顺序的情况下也可以正确的被渲染。深度缓冲是用来解决物体可见性的问题，基本思想是：根据深度缓存里的值判断这个物体距离摄像机的距离。开始渲染一个片元的时候，需要把它的深度值和已存在于深度缓存中的值作比较，如果它的值距离摄像机更远那么就不会被渲染到屏幕上。否则更新片元的深度值到深度缓存中。 透明效果我们可以不关心不透明物体的渲染顺序，因为在深度测试中就可以测试出物体离摄像机的距离再判断是否写入颜色缓冲。但是对于不透明物体，就没这么简单了。想要达到半透明的效果，我们要利用透明度混合。 透明度混合透明度混合要关闭深度写入。这是因为：假如一个半透明物体在一个不透明物体的前面，如果开启深度写入的话，距离摄像机更远的不透明物体就会被剔除，但是依照常理我们是可以透过半透明的物体看到不透明的物体。但是这就破坏了深度缓冲的机制，这是非常不好但是不得不做的折中方法，也因此使得渲染顺序变得非常重要。（注意：关闭深度写入，但是没有关闭深度测试） 渲染顺序我们考虑两种情况： 既有半透明物体也有不透明物体：我们先渲染所有的不透明物体再渲染半透明物体 全是半透明物体：开启深度测试，关闭深度写入的情况下将半透明物体按照距离摄像机的远近从后往前渲染。 这里有一个小问题，深度缓冲中的值是像素级别的，而一个半透明物体很可能有非常多个像素，这么一来每一个像素的深度值都可能不一样，以此会产生 循环遮挡的情况。 为了规避上面的问题，常常会把大的模型分割成小的几块，这样即使出现渲染错误，也不会出现太出格的结果。 Unity设置的渲染序列类似之前Tags { &quot;RenderType&quot;=&quot;Opaque&quot; },我们可以用Queue标签来决定我们的模型是怎么渲染的。 队列名称 队列索引 索引描述 Background 1000 最早被渲染的队列，一般绘制背景元素 Geometry 2000 默认渲染队列，不透明物体渲染队列 AlphaTest 2450 需要透明度测试的物体在这个队列渲染 Transparent 3000 使用透明度混合的物体在这个队列渲染 Overlay 4000 最后被渲染的物体在这个队列，一般用于叠加效果 代码设置如果我们想要通过透明度混合来实现半透明效果，代码如下 SubShader { Tags { \"RenderType\"=\"Transparent\" } Pass { ZWrite Off ······ } } ZWrite Off 意味者关闭深度写入，或者可以： SubShader { Tags { \"RenderType\"=\"Transparent\" } ZWrite Off ······ Pass { } } 这样表示这个SubShader下的所有Pass都会关闭深度写入","link":"/2017/11/18/Unity Shader 入门（四）：透明效果知识储备/"},{"title":"Unity Shader 入门（二）：查看第一个Shader","text":"查看第一个shader上一节是理论知识的储备，如果你对细节部分感兴趣可以阅读更多的资料（Cg，HLSL，GLSL，OpenGl，DirectX的官方Doc等等），如果不求甚解的话，那我们就通过查看第一个shader来加深理解。我们在Unity中新建一个shader（Assets-&gt;Create-&gt;shader-&gt;standard surface shader）打开看发现里面已经有很多代码了。（版本Unity 2017.2.0f3） Shader \"Custom/NewSurfaceShader\" { Properties { _Color (\"Color\", Color) = (1,1,1,1) _MainTex (\"Albedo (RGB)\", 2D) = \"white\" {} _Glossiness (\"Smoothness\", Range(0,1)) = 0.5 _Metallic (\"Metallic\", Range(0,1)) = 0.0 } SubShader { Tags { \"RenderType\"=\"Opaque\" } LOD 200 CGPROGRAM // Physically based Standard lighting model, and enable shadows on all light types #pragma surface surf Standard fullforwardshadows // Use shader model 3.0 target, to get nicer looking lighting #pragma target 3.0 sampler2D _MainTex; struct Input { float2 uv_MainTex; }; half _Glossiness; half _Metallic; fixed4 _Color; // Add instancing support for this shader. You need to check 'Enable Instancing' on materials that use the shader. // See https://docs.unity3d.com/Manual/GPUInstancing.html for more information about instancing. // #pragma instancing_options assumeuniformscaling UNITY_INSTANCING_CBUFFER_START(Props) // put more per-instance properties here UNITY_INSTANCING_CBUFFER_END void surf (Input IN, inout SurfaceOutputStandard o) { // Albedo comes from a texture tinted by color fixed4 c = tex2D (_MainTex, IN.uv_MainTex) * _Color; o.Albedo = c.rgb; // Metallic and smoothness come from slider variables o.Metallic = _Metallic; o.Smoothness = _Glossiness; o.Alpha = c.a; } ENDCG } FallBack \"Diffuse\" } emmm…….即使你有编程的基础也可能看的一头雾水，不过没关系，我们现在来一个个拆解这段代码。 首先我们来看shader的一个大体结构: 一个shader包含多个属性（Properties)，然后是一个或多个的子着色器（SubShader)，在实际运行中，哪一个子着色器被使用是由运行的平台所决定的。每一个子着色器中包含一个或者多个的Pass。在计算着色时，平台先选择最优先可以使用的着色器，然后依次运行其中的Pass，然后得到输出的结果。最后指定一个FallBack，用来处理所有Subshader都不能运行的情况,一般FallBack的都是平台已经定义好的shader。 逐行代码查看我们打开刚刚新建的shader代码，开始逐行来看吧： Shader \"Custom/NewSurfaceShader\" { Properties { _Color (\"Color\", Color) = (1,1,1,1) _MainTex (\"Albedo (RGB)\", 2D) = \"white\" {} _Glossiness (\"Smoothness\", Range(0,1)) = 0.5 _Metallic (\"Metallic\", Range(0,1)) = 0.0 } 首先第一行Shader \"Custom/NewSurfaceShader\" Custom是自定义的shader默认的文件夹，如果你自己想要归类shader文件夹，就可以定义二级标题比如”Custom/MyShader/NewSurfaceShader”，这样NewSurfaceShader就归类在MyShader下啦。 接着一段代码块Properties { _Color (\"Color\", Color) = (1,1,1,1) _MainTex (\"Albedo (RGB)\", 2D) = \"white\" {} _Glossiness (\"Smoothness\", Range(0,1)) = 0.5 _Metallic (\"Metallic\", Range(0,1)) = 0.0 } 这里是shader的属性部分：属性的格式写作如下 _Name(&quot;Display Name&quot;, type) = defaultValue[{options}] _Name : 变量名，在之后的Shader代码中都用这个名字来获取该属性的内容 Display Name : 显示名，在Unity Inspector上显示的名字 type : 类型，可能的type所表示的内容有以下几种： defaultValue : 上面类型的默认值 options : 对于2D，或者Cube贴图有关，默认写一个空白的{}，例如下表 类型 说明 语法 Float 浮点数 _MyFloat(“Float”,Float)=3.5 Int 整型数 _MyInt(“Int”,Int)=1 Range(min,max) 一个介于最小值和最大值之间的浮点数 _MyRange(“Range”,Range(0.0,1.0))=0.5 Color RGBA（红绿蓝和透明度）四个量来定义的颜色 _MyColor(“Color”,Color)=(1,1,1,1) 2D 贴图信息 _My2D(“2D”,2D)=”white”{} Cube 立方纹理，由6张关联的2D贴图合在一起 _MyCube(“Cube”,Cube)=”bump”{} Vector 四维数 _MyVector(“Vector”,Vector)=(1,2,3,1) SubShader内部构造Tags：键值对Tags { \"RenderType\"=\"Opaque\" } tags用来告诉渲染器：何时以及怎样渲染这个对象。 标签名称 标签说明 例子 Queue 控制渲染顺序，保证不透明物体在透明物体之前渲染 Tags {“Queue”=”Transparent”} RenderType 对着色器分类，例如这是渲染透明的，这是渲染不透明的 Tags {“RenderType”=”Opaque”} DisableBatching 是否对该SubShader进行批处理 Tags {“DisableBatching”=”True”} ForceNoShadowCasting 该SubShader是否会投射阴影 Tags {“ForceNoShadowCasting”=”True”} IgnoreProjector 该SubShader是否会Project影响，常用于半透明物体 Tags {“IgnoreProjector”=”True”} CanUseSpriteAtlas 该SubShader用于Sprites时，要设置成false Tags {“CanUseSpriteAtlas”=”False”} PreviewType Inspector preview上默认是圆形预设，可以改为plane或者skybox Tags {“PreviewType”=”Plane”} LOD：Level of DetailLOD 200 这个数值决定了我们能用什么样的Shader。当设定的LOD小于SubShader所指定的LOD时，这个SubShader就不可以用了。Unity自定义了一组LOD的数值，我们在实现自己的Shader的时候可以参考来设定自己的LOD数值，以便控制渲染。 LOD名称 数值 VertexLit及其系列 100 Decal, Reflective VertexLit 150 Diffuse 200 Diffuse Detail, Reflective Bumped Unlit, Reflective Bumped VertexLit 250 Bumped, Specular 300 Parallax 500 Parallax Specular 600 实现代码CGPROGRAM // Physically based Standard lighting model, and enable shadows on all light types #pragma surface surf Standard fullforwardshadows // Use shader model 3.0 target, to get nicer looking lighting #pragma target 3.0 sampler2D _MainTex; struct Input { float2 uv_MainTex; }; half _Glossiness; half _Metallic; fixed4 _Color; // Add instancing support for this shader. You need to check 'Enable Instancing' on materials that use the shader. // See https://docs.unity3d.com/Manual/GPUInstancing.html for more information about instancing. // #pragma instancing_options assumeuniformscaling UNITY_INSTANCING_CBUFFER_START(Props) // put more per-instance properties here UNITY_INSTANCING_CBUFFER_END void surf (Input IN, inout SurfaceOutputStandard o) { // Albedo comes from a texture tinted by color fixed4 c = tex2D (_MainTex, IN.uv_MainTex) * _Color; o.Albedo = c.rgb; // Metallic and smoothness come from slider variables o.Metallic = _Metallic; o.Smoothness = _Glossiness; o.Alpha = c.a; } ENDCG 终于到了最重要的部分，首先CGPROGRAM和ENDCG成对出现,表示中间包裹的是一段Cg程序，接着是一个编译指令：#pragma surface surf Standard fullforwardshadows意味着我们要写一个表面Shader，并指定了光照模型，具体语法是 #pragma surface surfaceFunction lightModel [optionalparams] surface ： 声明的是一个表面着色器 surfaceFunction ： 着色器代码的方法的名字 lightModel ： 使用的光照模型。 对应上面的编译指令：我们声明了一个表面着色器，实际的代码在 surf 函数中（在下面的代码能找到该函数），使用 Standard 作为光照模型。 接下来是 sampler2D _MainTex; 我们知道在CG中，Texture（贴图）简单来说就是一块内存存储的，使用了RGBA通道，且每个通道8bits，的数据。而具体地想知道像素与坐标的对应关系，以及获取这些数据，一次一次去计算内存地址或者偏移显然不可行，因此可以通过sampler2D来对贴图进行操作。一言以蔽之就是，sampler2D是GLSL中的2D贴图的类型，相应的，还有sampler1D，sampler3D，samplerCube等等格式。 然后的重点是：为什么在这里需要一句对_MainTex的声明？首先之前我们已经在Properties里声明过它是贴图了（_MainTex (&quot;Albedo (RGB)&quot;, 2D) = &quot;white&quot; {}）。我们用来实例的这个shader其实是由两个相对独立的块组成的，外层的属性声明，回滚等等是Unity可以直接使用和编译的ShaderLab；而现在我们是在CGPROGRAM…ENDCG这样一个代码块中，这是一段CG程序。对于这段CG程序，要想访问在Properties中所定义的变量的话，必须使用和之前变量相同的名字进行声明。因此sampler2D _MainTex;做的事情就是再次声明并链接了_MainTex，使得接下来的CG程序能够使用这个变量。后面的half _Glossiness; half _Metallic; fixed4 _Color;都是同样的道理。回到原来的地方，下一句是: struct Input { float2 uv_MainTex; }; 如果你有编程的经历，那么结构体应该很熟悉了，这一段我们结合下面的surf一起来说 void surf (Input IN, inout SurfaceOutputStandard o) { // Albedo comes from a texture tinted by color fixed4 c = tex2D (_MainTex, IN.uv_MainTex) * _Color; o.Albedo = c.rgb; // Metallic and smoothness come from slider variables o.Metallic = _Metallic; o.Smoothness = _Glossiness; o.Alpha = c.a; } 刚才提到的#pragma surface surf Standard fullforwardshadows里面surf 函数就是对应的上面一段。我们看函数头输入的参数有Input IN。这个Input就对应上面的结构体。我们可以把所需要参与计算的数据都放到这个Input结构中，再传入surf函数使用；SurfaceOutputStandard是已经定义好了里面类型输出结构。作为输入的结构体必须命名为Input，这个结构体中定义了一个float2的变量，emmmm···你可能会感到奇怪float后面跟着数字，这是什么意思呢？其实float和vec都可以在之后加入一个2到4的数字，来表示被打包在一起的2到4个同类型数。比如：float4 color; float3 multipliedColor = color.rgb * coordinate.x;之类的。 在这个例子里，我们声明了一个叫做uv_MainTex的包含两个浮点数的变量。UV mapping的作用是将一个2D贴图上的点按照一定规则映射到3D模型上，在CG程序中，我们有这样的约定，在一个贴图变量之前加上uv两个字母，就代表提取它的uv值。我们之后就可以在surf程序中直接通过访问uv_MainTex来取得这张贴图当前需要计算的点的坐标值。接下来我们详细看surf内部的操作： fixed4 c = tex2D (_MainTex, IN.uv_MainTex) * _Color; 这里用到了一个tex2d函数，这是CG程序中用来在一张贴图中对一个点进行采样的方法，返回一个float4。这个例子中用刚刚得到的float4*_Color使得这个贴图经过和颜色混合。 o.Albedo = c.rgb; 将其颜色的rbg值赋予了输出的像素颜色 o.Metallic = _Metallic; o.Smoothness = _Glossiness; 都是用到上头Properties中我们定义的变量来赋值材质中的Metallic and smoothness o.Alpha = c.a; 将a值赋予透明度。至此surf介绍完毕，这个例子中shader最重要的部分就是以上这些啦！ 最后一步FallBack \"Diffuse\" 当所有上面的SubShader都不可以在目标平台上运行时，Unity就会调用这个shader，当然你也可以关闭这个选项，那就意味着如果没有显卡可以跑上面的shader，那我们就不管它啦! 结语这是最简单最简单的shader，看到这里的你应该可以了解一些简单的shader了，可以去Unity的Surface Shader Exampless上查看一些基础shader的编写内容，下一篇我们会开始第一个shader的编写。","link":"/2017/09/12/Unity Shader 入门（二）：查看第一个Shader/"},{"title":"Unity Shader 入门（六）：模型描边Shader","text":"导语前面几篇我们写了几个边缘发光的shader，另外一个类似功能的就是模型描边，和边缘发光不同的地方在于，描边是在原有模型的基础上，添加一圈的外框。 老规矩还是来看一下效果图： 具体实现说明一下这个Shader的具体实现： 实现原理：Mesh Doubling (复制网格)： 需要一个单独的Pass来实现，重新绘制一个将所有表面都沿着法线方向延展模型，挤出一点点，然后将正面剪裁掉，只输出描边的颜色； 第二个Pass就是一个正常着色的Pass 具体解说：先放一段实现的代码： Shader \"Custom/Rim/RimLighting\" { Properties{ _MainColor(\"Main Color\", Color) = (1,1,1,1) _OutlineCol(\"OutlineCol\", Color) = (1,0,0,1) _OutlineFactor(\"OutlineFactor\", Range(0,1)) = 0.1 _MainTex(\"Base 2D\", 2D) = \"white\"{} } SubShader { //描边使用两个Pass，第一个pass沿法线挤出一点，只输出描边的颜色 Pass { Cull Front CGPROGRAM #include \"UnityCG.cginc\" fixed4 _OutlineCol; float _OutlineFactor; struct v2f { float4 pos : SV_POSITION; }; v2f vert(appdata_full v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); //将法线方向转换到视空间 float3 vnormal = mul((float3x3)UNITY_MATRIX_IT_MV, v.normal); //将视空间法线xy坐标转化到投影空间，只有xy需要，z深度不需要了 float2 offset = TransformViewToProjection(vnormal.xy); //在最终投影阶段输出进行偏移操作 o.pos.xy += offset * _OutlineFactor; return o; } fixed4 frag(v2f i) : SV_Target { //这个Pass直接输出描边颜色 return _OutlineCol; } //使用vert函数和frag函数 #pragma vertex vert #pragma fragment frag ENDCG } //正常着色的Pass Pass { CGPROGRAM //引入头文件 #include \"Lighting.cginc\" //使用vert函数和frag函数 #pragma vertex vert #pragma fragment frag //定义Properties中的变量 fixed4 _MainColor; sampler2D _MainTex; //定义结构体：vertex shader阶段输出的内容 struct v2f { float4 pos : SV_POSITION; float3 worldNormal : TEXCOORD0; }; //定义顶点shader,参数直接使用appdata_base（包含position, noramal, texcoord） v2f vert(appdata_base v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); //通过TRANSFORM_TEX宏转化纹理坐标，主要处理了Offset和Tiling的改变 o.worldNormal = mul(v.normal, (float3x3)unity_WorldToObject); return o; } //定义片元shader fixed4 frag(v2f i) : SV_Target { //unity自身的diffuse也是带了环境光，这里我们也增加一下环境光 fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * _MainColor.xyz; //归一化法线，即使在vert归一化也不行，从vert到frag阶段有差值处理，传入的法线方向并不是vertex shader直接传出的 fixed3 worldNormal = normalize(i.worldNormal); //把光照方向归一化 fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz); //根据半兰伯特模型计算像素的光照信息 fixed3 lambert = 0.5 * dot(worldNormal, worldLightDir) + 0.5; //最终输出颜色为lambert光强*材质diffuse颜色*光颜色 fixed3 diffuse = lambert * _MainColor.xyz * _LightColor0.xyz + ambient; //进行纹理采样 fixed4 color = _MainColor; color.rgb = color.rgb* diffuse; return fixed4(color); } ENDCG } } FallBack \"Diffuse\" } 详细的实现，包含在注释之中了。 包含问题但是这个实现方法有一个问题：线条并不连续，在平滑表面的表现尚可（球体，胶囊体等等），但是在锐利的表面上经常会出现断层（比如立方体等等）。还是利用Mesh Doubling (复制网格)的方法，但是不再简单只通过法线方向，而是：不严格地按照表面沿着法线的方向延展, 而是在标准化的点位置和法线方向之间取一个恰当的参数来做插值。 更新方案修改描边Pass的vert函数： v2f vert(appdata_full v) { v2f o; o.pos = UnityObjectToClipPos ( v.vertex ); float3 vnormal1 = normalize ( v.vertex.xyz ); //将法线方向转换到视空间 float3 vnormal2 = mul((float3x3)UNITY_MATRIX_IT_MV, v.normal); vnormal1 = lerp ( vnormal1, vnormal2, _Factor ); vnormal1 = mul ( ( float3x3 ) UNITY_MATRIX_IT_MV, vnormal1); float2 offset = TransformViewToProjection (vnormal1.xy ); offset = normalize ( offset ); float dist = distance ( mul ( UNITY_MATRIX_M, v.vertex ), _WorldSpaceCameraPos ); o.pos.xy += offset *_OutlineFactor; return o; } 其中的_Factor就是用来计算差值的参数，这个可以根据自己调节lerp ( vnormal1, vnormal2, _Factor ) 效果是： 最后上一个完整的修复过的Shader方案： Shader \"Custom/Rim/RimLightingFix\" { Properties{ _MainColor(\"Main Color\", Color) = (1,1,1,1) _OutlineCol(\"OutlineCol\", Color) = (1,0,0,1) _OutlineFactor(\"OutlineFactor\", Range(0,1)) = 0.1 _MainTex(\"Base 2D\", 2D) = \"white\"{} _Factor(\"Control Factor\",Range(0,1)) = 0.1 } SubShader { //描边使用两个Pass，第一个pass沿法线挤出一点，只输出描边的颜色 Pass{ Cull Front CGPROGRAM #include \"UnityCG.cginc\" //使用vert函数和frag函数 #pragma vertex vert #pragma fragment frag fixed4 _OutlineCol; float _OutlineFactor; float _Factor; struct v2f { float4 pos : SV_POSITION; }; v2f vert(appdata_full v) { v2f o; o.pos = UnityObjectToClipPos ( v.vertex ); float3 vnormal1 = normalize ( v.vertex.xyz ); //将法线方向转换到视空间 float3 vnormal2 = mul((float3x3)UNITY_MATRIX_IT_MV, v.normal); vnormal1 = lerp ( vnormal1, vnormal2, _Factor ); vnormal1 = mul ( ( float3x3 ) UNITY_MATRIX_IT_MV, vnormal1); float2 offset = TransformViewToProjection (vnormal1.xy ); offset = normalize ( offset ); float dist = distance ( mul ( UNITY_MATRIX_M, v.vertex ), _WorldSpaceCameraPos ); o.pos.xy += offset *_OutlineFactor; return o; } fixed4 frag(v2f i) : SV_Target { //这个Pass直接输出描边颜色 return _OutlineCol; } ENDCG } //正常着色的Pass Pass { CGPROGRAM //引入头文件 #include \"Lighting.cginc\" //使用vert函数和frag函数 #pragma vertex vert #pragma fragment frag //定义Properties中的变量 fixed4 _MainColor; sampler2D _MainTex; //定义结构体：vertex shader阶段输出的内容 struct v2f { float4 pos : SV_POSITION; float3 worldNormal : TEXCOORD0; }; //定义顶点shader,参数直接使用appdata_base（包含position, noramal, texcoord） v2f vert(appdata_base v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); //通过TRANSFORM_TEX宏转化纹理坐标，主要处理了Offset和Tiling的改变 o.worldNormal = mul(v.normal, (float3x3)unity_WorldToObject); return o; } //定义片元shader fixed4 frag(v2f i) : SV_Target { //unity自身的diffuse也是带了环境光，这里我们也增加一下环境光 fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * _MainColor.xyz; //归一化法线，即使在vert归一化也不行，从vert到frag阶段有差值处理，传入的法线方向并不是vertex shader直接传出的 fixed3 worldNormal = normalize(i.worldNormal); //把光照方向归一化 fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz); //根据半兰伯特模型计算像素的光照信息 fixed3 lambert = 0.5 * dot(worldNormal, worldLightDir) + 0.5; //最终输出颜色为lambert光强*材质diffuse颜色*光颜色 fixed3 diffuse = lambert * _MainColor.xyz * _LightColor0.xyz + ambient; //进行纹理采样 fixed4 color = _MainColor; color.rgb = color.rgb* diffuse; return fixed4(color); } ENDCG } } FallBack \"Diffuse\" } 结语描边常用于一些漫画风格的游戏场景中，能够在复杂的场景中突出被绘制的物体。","link":"/2018/02/08/Unity Shader 入门（六）：模型描边Shader/"}],"tags":[],"categories":[{"name":"Shader","slug":"Shader","link":"/categories/Shader/"}]}