{"pages":[],"posts":[{"title":"Unity Shader 入门（三）：编写第一个Shader","text":"编写第一个Shader上一节我们学习了第一个简单的Shader，现在我们可以开始写第一个shader练练手了（搓搓手）。首先我们挑一个【边缘发光（水晶球）】的shader来写 首先来看一下效果图，如果你感兴趣的话就接下来看吧： 实现原理：根据物体表面法向量和视线向量的夹角来判断是否是物体的边缘部位。夹角越大（接近垂直）说明越接近物体边缘部分。重点：向量点积运算。 具体解说：先放一段实现的代码：1234567891011121314151617181920212223242526272829303132333435363738Shader \"Custom/Rim//RimBump\" { Properties{ _Color(\"Main Color\", Color) = (1,1,1,1) _SpecColor(\"Specular Color\", Color) = (0.5, 0.5, 0.5, 1) _BumpMap(\"Normalmap\", 2D) = \"bump\" {} _RimColor(\"Rim Color\", Color) = (0.26,0.19,0.16,0.0) _RimPower(\"Rim Power\", Range(0.5,8.0)) = 2.0 } SubShader{ Tags { \"RenderType\" = \"Opaque\" } LOD 400 CGPROGRAM #pragma surface surf BlinnPhong #pragma target 3.0 sampler2D _BumpMap; fixed4 _Color; float4 _RimColor; float _RimPower; struct Input { float2 uv_MainTex; float2 uv_BumpMap; float3 viewDir; }; void surf(Input IN, inout SurfaceOutput o) { o.Albedo = _Color.rgb; o.Gloss = 1; o.Normal = UnpackNormal(tex2D(_BumpMap, IN.uv_BumpMap)); half rim = 1 - saturate(dot(normalize(IN.viewDir), o.Normal)); o.Emission = _RimColor.rgb * pow(rim, _RimPower); } ENDCG } FallBack \"Diffuse\"} 如果你看过上一篇的Shader介绍你应该可以大致看懂上面的代码，我们就关键部分说明一下：1234567void surf(Input IN, inout SurfaceOutput o) { o.Albedo = _Color.rgb; o.Gloss = 1; o.Normal = UnpackNormal(tex2D(_BumpMap, IN.uv_BumpMap)); half rim = 1 - saturate(dot(normalize(IN.viewDir), o.Normal)); o.Emission = _RimColor.rgb * pow(rim, _RimPower); } 首先这两句： 12o.Albedo = _Color.rgb;o.Gloss = 1; 类比上一篇，o.Albedo 此时可以获得我们设置的颜色和贴图之间混合后的颜色，o.Gloss 我们将发光强度设置成1。 接下来是重点：123o.Normal = UnpackNormal(tex2D(_BumpMap, IN.uv_BumpMap));half rim = 1 - saturate(dot(normalize(IN.viewDir), o.Normal));o.Emission = _RimColor.rgb * pow (rim, _RimStrength); UnpackNormal 是定义在UnityCG.cginc文件中的方法（这个文件中包含了一系列常用的CG变量以及方法，在Unity安装路径中可以找到），UnpackNormal接受一个fixed4的输入，并将其转换为所对应的法线值（fixed3）。在解包得到这个值之后，将其赋给输出的Normal，接下来我们就可以来使用Normal值啦。 有关法线贴图 这一点归类于扩展阅读，如果你想知道UnpackNormal的原理可以继续查看，如果不的话就跳过这一段吧！假设你想知道原理，那首先思考一个问题为什么法线贴图看起来大多是蓝色的？ 实际上，我们通常见到的这种偏蓝色的法线纹理中，存储的是在Tangent Space中的顶点法线方向。那么，问题又来了，什么是Tangent Space。在Tangent Space中，坐标原点就是顶点的位置，其中z轴是该顶点本身的法线方向（N）。这样，另外两个坐标轴就是和该点相切的两条切线。这样的切线是有无数条，但模型一般会给定该顶点的一个tangent。（给定的过程可以见这个链接） 通常我们所见的法线纹理是基于原法线信息构建的坐标系来构建出来的。那种偏蓝色的法线纹理其实就是存储在每个顶点各自的Tangent Space中法线的扰动方向。也就是说，如果一个顶点的法线方向不变，那么在它的Tangent Space中，新的normal值就是z轴方向，也就是说值为(0, 0, 1)。但这并不是法线纹理中存储的最终值：因为一个向量每个维度的取值范围在(-1, 1)，在法线贴图中被压缩在颜色的范围[0,1]中，所以需要转换：123&gt; 颜色 = 0.5 * 法线 + 0.5;&gt; 线 = 2 * (颜色 - 0.5); &gt; 这样，之前的法线值(0, 0, 1)实际上对应了法线纹理中RGB的值为(0.5, 0.5, 1)，而这个颜色也就是法线纹理中那大片的蓝色。这些蓝色实际上说明顶点的大部分法线是和模型本身法线一样的，不需要改变。总结一下就是，法线纹理的RGB通道存储了在每个顶点各自的Tangent Space中的法线方向的映射值。 下一个问题：Unity编辑器中加入一张发现贴图，编辑器都会提示把法线纹理的“Texture Type”设置成“Normal Map”，这是为什么呢？是因为这样的设置可以让Unity根据不同平台对纹理进行压缩，当需要法线信息时，再通过UnpackNormal函数对法线纹理进行正确的采样，即将把颜色通道变成一个适合于实时法向映射的格式。 再下一个问题：压缩的内容又是什么呢？其实法线贴图只有两个通道是真正必不可少的，因为第三个通道的值可以用另外两个推导出来（法线是单位向量）法线（x,y,z）是一条单位向量。所以知道了x,y,z里的任意两个，剩下的那个就可以通过计算得出。所以我们就可以使用2个通道的图储存x,y,z里的两个值，将xyz里剩余的值省略，通过计算得出。而压缩后的法线贴图，大小只有原来的1/4左右，故可以使用更大或者更多的贴图来提升画面品质。 重点讲解回到刚刚打断的地方，下面两句：12half rim = 1 - saturate(dot(normalize(IN.viewDir), o.Normal));o.Emission = _RimColor.rgb * pow (rim, _RimStrength); 首先我们看normalize函数：为了对向量进行归一化处理（这里传入IN.viewDir指的是：WorldSpace View Direction，也就是当前坐标的视角方向）。 dot函数：返回传入的两个参数的点积，saturate函数：判断传入的参数是否在0-1之间，如果小于0，返回 0；如果大于 1，返回1； 接着第二句：_RimColor.rgb * pow (rim, _RimStrength)从_RimColor参数获取自发光颜色再和发光的强度混合，最终将颜色赋值给像素的Emission（发散颜色） 以上就是边缘发光效果的实现。 结语下一次的shader我们将来写【半透明】的边缘发光效果。为此在下一篇我们将会梳理一下Unity shader透明效果的知识储配","link":"/2017/11/13/Unity Shader 入门（三）：编写第一个Shader/"},{"title":"Unity Shader 入门（五）：边缘发光透明版","text":"导语之前我们写过一个边缘发光的Shader（传送门），这一次我们来写这个的升级版：透明物体的边缘发光。 效果图首先我们还是来看一下效果图： Shader代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364Shader \"Custom/Rim/RimEnerge\" { Properties { _Color(\"Main Color\",Color) = (0.6,0.6,0.6,1) _AlphaRange(\"Alpha Range\",Range(0,1)) = 0 _RimColor(\"Rim Color\",Color) = (1,1,1,1) } SubShader { Tags{ \"Queue\"=\"Transparent\" \"IgnoreProjector\"=\"True\" \"RenderType\"=\"Transparent\" } ZWrite Off Blend SrcAlpha OneMinusSrcAlpha LOD 200 Pass { CGPROGRAM #pragma vertex vert #pragma fragment frag #include \"Lighting.cginc\" struct a2v { float4 vertex : POSITION; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float3 normalDir : TEXCOORD0; float3 worldPos : TEXCOORD1; }; fixed4 _Color; float _AlphaRange; fixed4 _RimColor; v2f vert( a2v v ) { v2f o; o.pos = UnityObjectToClipPos(v.vertex) ; o.normalDir = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld,v.vertex).xyz; return o; } fixed4 frag( v2f v ):COLOR { float3 normal = normalize(v.normalDir); float3 viewDir = normalize(_WorldSpaceCameraPos - v.worldPos); float normalDotViewDir = saturate(dot(normal,viewDir)); fixed3 diffuse = normalDotViewDir *_Color; return fixed4(diffuse + _RimColor ,(1 - normalDotViewDir) * (1 - _AlphaRange) + _AlphaRange); } ENDCG } } Fallback \"Diffuse\"} 透明度混合上一篇我们了解了透明度混合的原理以及一些透明度知识（传送门），而Unity中，为了进行透明度混合，我们需要用到【Blend】命令： 语法 描述 Blend Off 关闭混合（这是默认的状态） Blend SrcFactor DstFactor 开启混合，该片元产生的颜色SrcFactor. 已存在于屏幕的颜色 DstFactor，然后将两者叠加在一起存入颜色缓冲。 Blend SrcFactor DstFactor, SrcFactorA DstFactorA 原理同上，不过使用了不同的混合因子 BlendOp Op 不同于上面的颜色混合，而是使用Blend Operation（传送门）来对它们进行操作 BlendOp OpColor, OpAlpha 原理同上，不过采用不同的Blend Operation来操作Color和Alpha的通道 混合因子： 名称 描述 One 因子为1，表示让源颜色或者目标颜色通过 Zero 因子为0，用来删除源颜色或者目标颜色 SrcColor 因子为源颜色 SrcAlpha 因子为源颜色的透明度 DstColor 因子为目标颜色 DstAlpha 因子为目标颜色的透明度 OneMinusSrcColor 因子为 (1 - 源颜色) 的值 OneMinusSrcAlpha 因子为 (1 - 源颜色的透明度) 的值 OneMinusDstColor 因子为 (1 - 目标颜色) 的值 OneMinusDstAlpha 因子为 (1 - 目标颜色的透明度) 的值 此时我们再来看上面这一块代码：1234567Tags{ \"Queue\"=\"Transparent\" \"IgnoreProjector\"=\"True\" \"RenderType\"=\"Transparent\" } ZWrite Off Blend SrcAlpha OneMinusSrcAlpha LOD 200 这里有一些新的知识：之前提过半透明物体的渲染序列要设置成&quot;Queue&quot;=&quot;Transparent&quot;,而&quot;RenderType&quot;=&quot;Transparent&quot;表示我们使用了透明度混合。通常一个半透明的Shader Tags都包含这三条： 123\"Queue\"=\"Transparent\"\"IgnoreProjector\"=\"True\"\"RenderType\"=\"Transparent\" 接下来是 ZWrite Off : 我们在上一篇介绍过为什么透明度混合需要关闭深度写入 最后是 Blend SrcAlpha OneMinusSrcAlpha : 这里我们将源颜色的混合因子设置成SrcAlpha，将目标颜色的混合因子设置成 OneMinusSrcAlpha 以得到半透明效果。 结构体定义123456789101112struct a2v{ float4 vertex : POSITION; float3 normal : NORMAL; };struct v2f{ float4 pos : SV_POSITION; float3 normalDir : TEXCOORD0; float3 worldPos : TEXCOORD1;}; a2v ：包含顶点着色器要的模型数据 float4 vertex : POSITION;这一句表示：用模型顶点的坐标填充vertex变量。 float3 normal : NORMAL; 这一句表示：用模型空间的法线方向向量填充normal变量 v2f ：用于顶点着色器和片元着色器之间传递信息 float4 pos : SV_POSITION;这一句表示：用裁剪空间的位置信息填充pos变量 float3 normalDir : TEXCOORD0;这一句表示：用模型的第一套纹理坐标填充normalDir变量 float3 worldPos : TEXCOORD1;这一句表示：用模型的第二套纹理坐标填充worldPos变量 顶点着色器12345678v2f vert( a2v v ){ v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.normalDir = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld,v.vertex).xyz; return o;} UnityObjectToClipPos(v.vertex)是Unity5.6之后的写法，之前是mul(UNITY_MATRIX_MVP,v.vertex) 这一句的意思是:将模型空间的顶点信息转换到裁剪空间中的位置信息，然后将信息存储在o.pos中。 UnityObjectToWorldNormal(v.normal)这一句的意思是:法线从模型空间变换到世界空间中并计算物体在世界空间中的法线坐标。 mul(unity_ObjectToWorld,v.vertex).xyz;这一句的意思是：将顶点从模型空间转换到世界空间的信息存储到worldPos变量中。 片元着色器12345678 fixed4 frag( v2f v ):COLOR { float3 normal = normalize(v.normalDir); float3 viewDir = normalize(_WorldSpaceCameraPos - v.worldPos); float normalDotViewDir = saturate(dot(normal,viewDir));fixed3 diffuse = normalDotViewDir *_Color; return fixed4(diffuse + _RimColor ,(1 - normalDotViewDir) * (1 - _AlphaRange) + _AlphaRange); } fixed4 frag( v2f v ):COLOR 我们注意到片元着色器的后面跟着:COLOR ：这是Unity提供的Cg/HLSL语义。语义可以告诉shader数据的来源以及数据的输出。 float3 viewDir = normalize(_WorldSpaceCameraPos - v.worldPos); 这里我们用对象在世界坐标系中的位置减去摄像机的世界空间位置，并进行逐顶点归一化，赋给视线的方向 float normalDotViewDir = saturate(dot(normal,viewDir)) 我们获得法线与视线的夹角 fixed3 diffuse = normalDotViewDir *_Color; 这里我们视线与法线的夹角和主颜色相乘。 return fixed4(diffuse + _RimColor ,(1 - normalDotViewDir) * (1 - _AlphaRange) + _AlphaRange); 最后将混合后的颜色输出。","link":"/2017/11/25/Unity Shader 入门（五）：边缘发光透明版/"},{"title":"Unity Shader 入门（一）：理论准备（二）","text":"什么是Shader？Shader（着色器）：是渲染管线上的一小段程序，它负责将输入的Mesh（网格）以指定的方式和输入的贴图、颜色等组合作用然后输出。Shader开发者要做的就是根据输入，进行计算变换，产生输出。 shader大体上可以分为两类： 顶点着色器（Vertex Shader） 片元着色器（Fragment Shader） 而在Unity Shader中分为三类： Surface Shaders （表面着色器）：是Unity对Vertex/Fragment Shader的一层包装，可以以极少的代码来完成不同的光照模型与不同平台下需要考虑的事情；缺点是能够实现的效果不如片段着色器来的多。 Vertex/Fragment Shaders （顶点/片断着色器） Fixed Function Shaders （固定管线着色器）：已被淘汰 什么是ShaderLab？在Unity中，所有的Shader都是使用ShaderLab来编写的，从结构上来说，它定义了显示一个材质所需要的所有东西，而不仅仅是着色器代码，我们先来看一下ShaderLab的解构 一个shader包含多个属性（Properties)，然后是一个或多个的子着色器（SubShader)，在实际运行中，哪一个子着色器被使用是由运行的平台所决定的。每一个子着色器中包含一个或者多个的Pass。在计算着色时，平台先选择最优先可以使用的着色器，然后依次运行其中的Pass，然后得到输出的结果。最后指定一个FallBack，用来处理所有Subshader都不能运行的情况,一般FallBack的都是平台已经定义好的shader。","link":"/2019/04/29/Unity Shader 入门（一）：理论准备（二）/"},{"title":"Unity Shader 入门（四）：透明效果知识储备","text":"导语首先一个问题：如果场景中有非常多的物体，彼此之间有互相遮挡的情况，那么这些物体是按照什么样的渲染顺序进行渲染的呢？ 深度缓冲实际上，由于深度缓存（z-buffer）的存在,不透明的物体在不考虑渲染顺序的情况下也可以正确的被渲染。深度缓冲是用来解决物体可见性的问题，基本思想是：根据深度缓存里的值判断这个物体距离摄像机的距离。开始渲染一个片元的时候，需要把它的深度值和已存在于深度缓存中的值作比较，如果它的值距离摄像机更远那么就不会被渲染到屏幕上。否则更新片元的深度值到深度缓存中。 透明效果我们可以不关心不透明物体的渲染顺序，因为在深度测试中就可以测试出物体离摄像机的距离再判断是否写入颜色缓冲。但是对于不透明物体，就没这么简单了。想要达到半透明的效果，我们要利用透明度混合。 透明度混合透明度混合要关闭深度写入。这是因为：假如一个半透明物体在一个不透明物体的前面，如果开启深度写入的话，距离摄像机更远的不透明物体就会被剔除，但是依照常理我们是可以透过半透明的物体看到不透明的物体。但是这就破坏了深度缓冲的机制，这是非常不好但是不得不做的折中方法，也因此使得渲染顺序变得非常重要。（注意：关闭深度写入，但是没有关闭深度测试） 渲染顺序我们考虑两种情况： 既有半透明物体也有不透明物体：我们先渲染所有的不透明物体再渲染半透明物体 全是半透明物体：开启深度测试，关闭深度写入的情况下将半透明物体按照距离摄像机的远近从后往前渲染。 这里有一个小问题，深度缓冲中的值是像素级别的，而一个半透明物体很可能有非常多个像素，这么一来每一个像素的深度值都可能不一样，以此会产生 循环遮挡的情况。 为了规避上面的问题，常常会把大的模型分割成小的几块，这样即使出现渲染错误，也不会出现太出格的结果。 Unity设置的渲染序列类似之前Tags { &quot;RenderType&quot;=&quot;Opaque&quot; },我们可以用Queue标签来决定我们的模型是怎么渲染的。 队列名称 队列索引 索引描述 Background 1000 最早被渲染的队列，一般绘制背景元素 Geometry 2000 默认渲染队列，不透明物体渲染队列 AlphaTest 2450 需要透明度测试的物体在这个队列渲染 Transparent 3000 使用透明度混合的物体在这个队列渲染 Overlay 4000 最后被渲染的物体在这个队列，一般用于叠加效果 代码设置如果我们想要通过透明度混合来实现半透明效果，代码如下1234567SubShader { Tags { \"RenderType\"=\"Transparent\" } Pass { ZWrite Off ······ } } ZWrite Off 意味者关闭深度写入，或者可以：123456SubShader { Tags { \"RenderType\"=\"Transparent\" } ZWrite Off ······ Pass { } } 这样表示这个SubShader下的所有Pass都会关闭深度写入","link":"/2017/11/18/Unity Shader 入门（四）：透明效果知识储备/"},{"title":"Unity Shader 入门（二）：查看第一个Shader","text":"查看第一个shader上一节是理论知识的储备，如果你对细节部分感兴趣可以阅读更多的资料（Cg，HLSL，GLSL，OpenGl，DirectX的官方Doc等等），如果不求甚解的话，那我们就通过查看第一个shader来加深理解。我们在Unity中新建一个shader（Assets-&gt;Create-&gt;shader-&gt;standard surface shader）打开看发现里面已经有很多代码了。（版本Unity 2017.2.0f3） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748Shader \"Custom/NewSurfaceShader\" { Properties { _Color (\"Color\", Color) = (1,1,1,1) _MainTex (\"Albedo (RGB)\", 2D) = \"white\" {} _Glossiness (\"Smoothness\", Range(0,1)) = 0.5 _Metallic (\"Metallic\", Range(0,1)) = 0.0 } SubShader { Tags { \"RenderType\"=\"Opaque\" } LOD 200 CGPROGRAM // Physically based Standard lighting model, and enable shadows on all light types #pragma surface surf Standard fullforwardshadows // Use shader model 3.0 target, to get nicer looking lighting #pragma target 3.0 sampler2D _MainTex; struct Input { float2 uv_MainTex; }; half _Glossiness; half _Metallic; fixed4 _Color; // Add instancing support for this shader. You need to check 'Enable Instancing' on materials that use the shader. // See https://docs.unity3d.com/Manual/GPUInstancing.html for more information about instancing. // #pragma instancing_options assumeuniformscaling UNITY_INSTANCING_CBUFFER_START(Props) // put more per-instance properties here UNITY_INSTANCING_CBUFFER_END void surf (Input IN, inout SurfaceOutputStandard o) { // Albedo comes from a texture tinted by color fixed4 c = tex2D (_MainTex, IN.uv_MainTex) * _Color; o.Albedo = c.rgb; // Metallic and smoothness come from slider variables o.Metallic = _Metallic; o.Smoothness = _Glossiness; o.Alpha = c.a; } ENDCG } FallBack \"Diffuse\"} emmm…….即使你有编程的基础也可能看的一头雾水，不过没关系，我们现在来一个个拆解这段代码。 逐行代码查看我们打开刚刚新建的shader代码，开始逐行来看吧： 1234567Shader \"Custom/NewSurfaceShader\" { Properties { _Color (\"Color\", Color) = (1,1,1,1) _MainTex (\"Albedo (RGB)\", 2D) = \"white\" {} _Glossiness (\"Smoothness\", Range(0,1)) = 0.5 _Metallic (\"Metallic\", Range(0,1)) = 0.0 } 首先第一行1Shader \"Custom/NewSurfaceShader\" Custom是自定义的shader默认的文件夹，如果你自己想要归类shader文件夹，就可以定义二级标题比如”Custom/MyShader/NewSurfaceShader”，这样NewSurfaceShader就归类在MyShader下啦。 接着一段代码块123456Properties { _Color (\"Color\", Color) = (1,1,1,1) _MainTex (\"Albedo (RGB)\", 2D) = \"white\" {} _Glossiness (\"Smoothness\", Range(0,1)) = 0.5 _Metallic (\"Metallic\", Range(0,1)) = 0.0 } 这里是shader的属性部分：属性的格式写作如下123456789101112131415161718192021222324_Name(&quot;Display Name&quot;, type) = defaultValue[{options}]``` - _Name : 变量名，在之后的Shader代码中都用这个名字来获取该属性的内容- Display Name : 显示名，在Unity Inspector上显示的名字- type : 类型，可能的type所表示的内容有以下几种： - defaultValue : 上面类型的默认值- options : 对于2D，或者Cube贴图有关，默认写一个空白的{}，例如下表类型|说明|语法:--:|:--:|:--:|Float|浮点数|_MyFloat(&quot;Float&quot;,Float)=3.5Int|整型数|_MyInt(&quot;Int&quot;,Int)=1Range(min,max)|一个介于最小值和最大值之间的浮点数|_MyRange(&quot;Range&quot;,Range(0.0,1.0))=0.5Color|RGBA（红绿蓝和透明度）四个量来定义的颜色|_MyColor(&quot;Color&quot;,Color)=(1,1,1,1)2D|贴图信息|_My2D(&quot;2D&quot;,2D)=&quot;white&quot;{}Cube|立方纹理，由6张关联的2D贴图合在一起|_MyCube(&quot;Cube&quot;,Cube)=&quot;bump&quot;{}Vector|四维数|_MyVector(&quot;Vector&quot;,Vector)=(1,2,3,1)### SubShader内部构造#### Tags：键值对```haxeTags { &quot;RenderType&quot;=&quot;Opaque&quot; } tags用来告诉渲染器：何时以及怎样渲染这个对象。 标签名称 标签说明 例子 Queue 控制渲染顺序，保证不透明物体在透明物体之前渲染 Tags {“Queue”=”Transparent”} RenderType 对着色器分类，例如这是渲染透明的，这是渲染不透明的 Tags {“RenderType”=”Opaque”} DisableBatching 是否对该SubShader进行批处理 Tags {“DisableBatching”=”True”} ForceNoShadowCasting 该SubShader是否会投射阴影 Tags {“ForceNoShadowCasting”=”True”} IgnoreProjector 该SubShader是否会Project影响，常用于半透明物体 Tags {“IgnoreProjector”=”True”} CanUseSpriteAtlas 该SubShader用于Sprites时，要设置成false Tags {“CanUseSpriteAtlas”=”False”} PreviewType Inspector preview上默认是圆形预设，可以改为plane或者skybox Tags {“PreviewType”=”Plane”} LOD：Level of Detail1LOD 200 这个数值决定了我们能用什么样的Shader。当设定的LOD小于SubShader所指定的LOD时，这个SubShader就不可以用了。Unity自定义了一组LOD的数值，我们在实现自己的Shader的时候可以参考来设定自己的LOD数值，以便控制渲染。 LOD名称 数值 VertexLit及其系列 100 Decal, Reflective VertexLit 150 Diffuse 200 Diffuse Detail, Reflective Bumped Unlit, Reflective Bumped VertexLit 250 Bumped, Specular 300 Parallax 500 Parallax Specular 600 实现代码12345678910111213141516171819202122232425262728CGPROGRAM // Physically based Standard lighting model, and enable shadows on all light types #pragma surface surf Standard fullforwardshadows // Use shader model 3.0 target, to get nicer looking lighting #pragma target 3.0 sampler2D _MainTex; struct Input { float2 uv_MainTex; }; half _Glossiness; half _Metallic; fixed4 _Color; // Add instancing support for this shader. You need to check 'Enable Instancing' on materials that use the shader. // See https://docs.unity3d.com/Manual/GPUInstancing.html for more information about instancing. // #pragma instancing_options assumeuniformscaling UNITY_INSTANCING_CBUFFER_START(Props) // put more per-instance properties here UNITY_INSTANCING_CBUFFER_END void surf (Input IN, inout SurfaceOutputStandard o) { // Albedo comes from a texture tinted by color fixed4 c = tex2D (_MainTex, IN.uv_MainTex) * _Color; o.Albedo = c.rgb; // Metallic and smoothness come from slider variables o.Metallic = _Metallic; o.Smoothness = _Glossiness; o.Alpha = c.a; } ENDCG 终于到了最重要的部分，首先CGPROGRAM和ENDCG成对出现,表示中间包裹的是一段Cg程序，接着是一个编译指令：#pragma surface surf Standard fullforwardshadows意味着我们要写一个表面Shader，并指定了光照模型，具体语法是1#pragma surface surfaceFunction lightModel [optionalparams] surface ： 声明的是一个表面着色器 surfaceFunction ： 着色器代码的方法的名字 lightModel ： 使用的光照模型。 对应上面的编译指令：我们声明了一个表面着色器，实际的代码在 surf 函数中（在下面的代码能找到该函数），使用 Standard 作为光照模型。 接下来是 sampler2D _MainTex; 我们知道在CG中，Texture（贴图）简单来说就是一块内存存储的，使用了RGBA通道，且每个通道8bits，的数据。而具体地想知道像素与坐标的对应关系，以及获取这些数据，一次一次去计算内存地址或者偏移显然不可行，因此可以通过sampler2D来对贴图进行操作。一言以蔽之就是，sampler2D是GLSL中的2D贴图的类型，相应的，还有sampler1D，sampler3D，samplerCube等等格式。 然后的重点是：为什么在这里需要一句对_MainTex的声明？首先之前我们已经在Properties里声明过它是贴图了（_MainTex (&quot;Albedo (RGB)&quot;, 2D) = &quot;white&quot; {}）。我们用来实例的这个shader其实是由两个相对独立的块组成的，外层的属性声明，回滚等等是Unity可以直接使用和编译的ShaderLab；而现在我们是在CGPROGRAM…ENDCG这样一个代码块中，这是一段CG程序。对于这段CG程序，要想访问在Properties中所定义的变量的话，必须使用和之前变量相同的名字进行声明。因此sampler2D _MainTex;做的事情就是再次声明并链接了_MainTex，使得接下来的CG程序能够使用这个变量。后面的half _Glossiness; half _Metallic; fixed4 _Color;都是同样的道理。回到原来的地方，下一句是:123struct Input { float2 uv_MainTex;}; 如果你有编程的经历，那么结构体应该很熟悉了，这一段我们结合下面的surf一起来说123456789void surf (Input IN, inout SurfaceOutputStandard o) { // Albedo comes from a texture tinted by color fixed4 c = tex2D (_MainTex, IN.uv_MainTex) * _Color; o.Albedo = c.rgb; // Metallic and smoothness come from slider variables o.Metallic = _Metallic; o.Smoothness = _Glossiness; o.Alpha = c.a; } 刚才提到的#pragma surface surf Standard fullforwardshadows里面surf 函数就是对应的上面一段。我们看函数头输入的参数有Input IN。这个Input就对应上面的结构体。我们可以把所需要参与计算的数据都放到这个Input结构中，再传入surf函数使用；SurfaceOutputStandard是已经定义好了里面类型输出结构。作为输入的结构体必须命名为Input，这个结构体中定义了一个float2的变量，emmmm···你可能会感到奇怪float后面跟着数字，这是什么意思呢？其实float和vec都可以在之后加入一个2到4的数字，来表示被打包在一起的2到4个同类型数。比如：float4 color; float3 multipliedColor = color.rgb * coordinate.x;之类的。 在这个例子里，我们声明了一个叫做uv_MainTex的包含两个浮点数的变量。UV mapping的作用是将一个2D贴图上的点按照一定规则映射到3D模型上，在CG程序中，我们有这样的约定，在一个贴图变量之前加上uv两个字母，就代表提取它的uv值。我们之后就可以在surf程序中直接通过访问uv_MainTex来取得这张贴图当前需要计算的点的坐标值。接下来我们详细看surf内部的操作：1fixed4 c = tex2D (_MainTex, IN.uv_MainTex) * _Color; 这里用到了一个tex2d函数，这是CG程序中用来在一张贴图中对一个点进行采样的方法，返回一个float4。这个例子中用刚刚得到的float4*_Color使得这个贴图经过和颜色混合。1o.Albedo = c.rgb; 将其颜色的rbg值赋予了输出的像素颜色12o.Metallic = _Metallic;o.Smoothness = _Glossiness; 都是用到上头Properties中我们定义的变量来赋值材质中的Metallic and smoothness 1o.Alpha = c.a; 将a值赋予透明度。至此surf介绍完毕，这个例子中shader最重要的部分就是以上这些啦！ 最后一步1FallBack \"Diffuse\" 当所有上面的SubShader都不可以在目标平台上运行时，Unity就会调用这个shader，当然你也可以关闭这个选项，那就意味着如果没有显卡可以跑上面的shader，那我们就不管它啦! 结语这是最简单最简单的shader，看到这里的你应该可以了解一些简单的shader了，可以去Unity的Surface Shader Exampless上查看一些基础shader的编写内容，下一篇我们会开始第一个shader的编写。","link":"/2017/09/12/Unity Shader 入门（二）：查看第一个Shader/"},{"title":"Unity Shader 入门（六）：模型描边Shader","text":"导语前面几篇我们写了几个边缘发光的shader，另外一个类似功能的就是模型描边，和边缘发光不同的地方在于，描边是在原有模型的基础上，添加一圈的外框。 老规矩还是来看一下效果图： 具体实现说明一下这个Shader的具体实现： 实现原理：Mesh Doubling (复制网格)： 需要一个单独的Pass来实现，重新绘制一个将所有表面都沿着法线方向延展模型，挤出一点点，然后将正面剪裁掉，只输出描边的颜色； 第二个Pass就是一个正常着色的Pass 具体解说：先放一段实现的代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798Shader \"Custom/Rim/RimLighting\" { Properties{ _MainColor(\"Main Color\", Color) = (1,1,1,1) _OutlineCol(\"OutlineCol\", Color) = (1,0,0,1) _OutlineFactor(\"OutlineFactor\", Range(0,1)) = 0.1 _MainTex(\"Base 2D\", 2D) = \"white\"{} } SubShader { //描边使用两个Pass，第一个pass沿法线挤出一点，只输出描边的颜色 Pass { Cull Front CGPROGRAM #include \"UnityCG.cginc\" fixed4 _OutlineCol; float _OutlineFactor; struct v2f { float4 pos : SV_POSITION; }; v2f vert(appdata_full v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); //将法线方向转换到视空间 float3 vnormal = mul((float3x3)UNITY_MATRIX_IT_MV, v.normal); //将视空间法线xy坐标转化到投影空间，只有xy需要，z深度不需要了 float2 offset = TransformViewToProjection(vnormal.xy); //在最终投影阶段输出进行偏移操作 o.pos.xy += offset * _OutlineFactor; return o; } fixed4 frag(v2f i) : SV_Target { //这个Pass直接输出描边颜色 return _OutlineCol; } //使用vert函数和frag函数 #pragma vertex vert #pragma fragment frag ENDCG } //正常着色的Pass Pass { CGPROGRAM //引入头文件 #include \"Lighting.cginc\" //使用vert函数和frag函数 #pragma vertex vert #pragma fragment frag //定义Properties中的变量 fixed4 _MainColor; sampler2D _MainTex; //定义结构体：vertex shader阶段输出的内容 struct v2f { float4 pos : SV_POSITION; float3 worldNormal : TEXCOORD0; }; //定义顶点shader,参数直接使用appdata_base（包含position, noramal, texcoord） v2f vert(appdata_base v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); //通过TRANSFORM_TEX宏转化纹理坐标，主要处理了Offset和Tiling的改变 o.worldNormal = mul(v.normal, (float3x3)unity_WorldToObject); return o; } //定义片元shader fixed4 frag(v2f i) : SV_Target { //unity自身的diffuse也是带了环境光，这里我们也增加一下环境光 fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * _MainColor.xyz; //归一化法线，即使在vert归一化也不行，从vert到frag阶段有差值处理，传入的法线方向并不是vertex shader直接传出的 fixed3 worldNormal = normalize(i.worldNormal); //把光照方向归一化 fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz); //根据半兰伯特模型计算像素的光照信息 fixed3 lambert = 0.5 * dot(worldNormal, worldLightDir) + 0.5; //最终输出颜色为lambert光强*材质diffuse颜色*光颜色 fixed3 diffuse = lambert * _MainColor.xyz * _LightColor0.xyz + ambient; //进行纹理采样 fixed4 color = _MainColor; color.rgb = color.rgb* diffuse; return fixed4(color); } ENDCG } } FallBack \"Diffuse\"} 详细的实现，包含在注释之中了。 包含问题但是这个实现方法有一个问题：线条并不连续，在平滑表面的表现尚可（球体，胶囊体等等），但是在锐利的表面上经常会出现断层（比如立方体等等）。还是利用Mesh Doubling (复制网格)的方法，但是不再简单只通过法线方向，而是：不严格地按照表面沿着法线的方向延展, 而是在标准化的点位置和法线方向之间取一个恰当的参数来做插值。 更新方案修改描边Pass的vert函数： 123456789101112131415v2f vert(appdata_full v) { v2f o; o.pos = UnityObjectToClipPos ( v.vertex ); float3 vnormal1 = normalize ( v.vertex.xyz ); //将法线方向转换到视空间 float3 vnormal2 = mul((float3x3)UNITY_MATRIX_IT_MV, v.normal); vnormal1 = lerp ( vnormal1, vnormal2, _Factor ); vnormal1 = mul ( ( float3x3 ) UNITY_MATRIX_IT_MV, vnormal1); float2 offset = TransformViewToProjection (vnormal1.xy ); offset = normalize ( offset ); float dist = distance ( mul ( UNITY_MATRIX_M, v.vertex ), _WorldSpaceCameraPos ); o.pos.xy += offset *_OutlineFactor; return o; } 其中的_Factor就是用来计算差值的参数，这个可以根据自己调节lerp ( vnormal1, vnormal2, _Factor ) 效果是： 最后上一个完整的修复过的Shader方案： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101Shader \"Custom/Rim/RimLightingFix\" { Properties{ _MainColor(\"Main Color\", Color) = (1,1,1,1) _OutlineCol(\"OutlineCol\", Color) = (1,0,0,1) _OutlineFactor(\"OutlineFactor\", Range(0,1)) = 0.1 _MainTex(\"Base 2D\", 2D) = \"white\"{} _Factor(\"Control Factor\",Range(0,1)) = 0.1 } SubShader { //描边使用两个Pass，第一个pass沿法线挤出一点，只输出描边的颜色 Pass{ Cull Front CGPROGRAM #include \"UnityCG.cginc\" //使用vert函数和frag函数 #pragma vertex vert #pragma fragment frag fixed4 _OutlineCol; float _OutlineFactor; float _Factor; struct v2f { float4 pos : SV_POSITION; }; v2f vert(appdata_full v) { v2f o; o.pos = UnityObjectToClipPos ( v.vertex ); float3 vnormal1 = normalize ( v.vertex.xyz ); //将法线方向转换到视空间 float3 vnormal2 = mul((float3x3)UNITY_MATRIX_IT_MV, v.normal); vnormal1 = lerp ( vnormal1, vnormal2, _Factor ); vnormal1 = mul ( ( float3x3 ) UNITY_MATRIX_IT_MV, vnormal1); float2 offset = TransformViewToProjection (vnormal1.xy ); offset = normalize ( offset ); float dist = distance ( mul ( UNITY_MATRIX_M, v.vertex ), _WorldSpaceCameraPos ); o.pos.xy += offset *_OutlineFactor; return o; } fixed4 frag(v2f i) : SV_Target { //这个Pass直接输出描边颜色 return _OutlineCol; } ENDCG } //正常着色的Pass Pass { CGPROGRAM //引入头文件 #include \"Lighting.cginc\" //使用vert函数和frag函数 #pragma vertex vert #pragma fragment frag //定义Properties中的变量 fixed4 _MainColor; sampler2D _MainTex; //定义结构体：vertex shader阶段输出的内容 struct v2f { float4 pos : SV_POSITION; float3 worldNormal : TEXCOORD0; }; //定义顶点shader,参数直接使用appdata_base（包含position, noramal, texcoord） v2f vert(appdata_base v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); //通过TRANSFORM_TEX宏转化纹理坐标，主要处理了Offset和Tiling的改变 o.worldNormal = mul(v.normal, (float3x3)unity_WorldToObject); return o; } //定义片元shader fixed4 frag(v2f i) : SV_Target { //unity自身的diffuse也是带了环境光，这里我们也增加一下环境光 fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * _MainColor.xyz; //归一化法线，即使在vert归一化也不行，从vert到frag阶段有差值处理，传入的法线方向并不是vertex shader直接传出的 fixed3 worldNormal = normalize(i.worldNormal); //把光照方向归一化 fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz); //根据半兰伯特模型计算像素的光照信息 fixed3 lambert = 0.5 * dot(worldNormal, worldLightDir) + 0.5; //最终输出颜色为lambert光强*材质diffuse颜色*光颜色 fixed3 diffuse = lambert * _MainColor.xyz * _LightColor0.xyz + ambient; //进行纹理采样 fixed4 color = _MainColor; color.rgb = color.rgb* diffuse; return fixed4(color); } ENDCG } } FallBack \"Diffuse\"} 结语描边常用于一些漫画风格的游戏场景中，能够在复杂的场景中突出被绘制的物体。","link":"/2018/02/08/Unity Shader 入门（六）：模型描边Shader/"},{"title":"Unity Shader 入门（一）：理论准备（一）","text":"什么是渲染管线？Shader（着色器）：是渲染管线上的一小段程序，要了解Shader首先要明白渲染管线是什么呢？ 渲染管线的主要功能是在给定虚拟相机、三维物体、光源、照明模式，以及纹理等诸多条件的情况下，生成或绘制一幅二维图像的过程。对于实时渲染来说，渲染管线就是基础，可以说渲染管线是实时渲染的底层工具。 在概念上可以将图形渲染管线分为四个阶段： 应用程序阶段（The Application） 几何阶段（The Geometry） 光栅化阶段（The Rasterizer） 像素处理阶段（pixel processing） 这里有几个要点： 每个阶段本身也可能是一条管线，如图中的几何阶段所示。此外，还可以对有的阶段进行全部或者部分的并行化处理，如图中的像素处理阶段。应用程序阶段虽然是一个单独的过程，但是依然可以对之进行管线化或者并行化处理。 最慢的管线阶段决定绘制速度，即图像的更新速度，这种速度一般用 frames per second(FPS) 来表示，也就是每秒绘制的图像数量，或者用 Hertz (Hz) 来表示。 应用程序阶段 The Application Stage顾名思义，应用程序阶段是由应用程序驱动的，一般是图形渲染管线概念上的第一个阶段。开发者能够对该阶段发生的情况进行完全控制，可以通过改变实现方法来改变实际性能，可以做的任务包括： 碰撞检测、输入检测、力反馈 纹理动画、变换仿真、几何形变 等等 在其他阶段是全部或者部分建立在硬件基础上，因此要改变实现过程会非常困难。 正因应用程序阶段是软件方式实现，因此不能像几何和光栅化阶段那样继续分为若干个子阶段。但为了提高性能，该阶段还是可以在几个并行处理器上同时执行。在 CPU 设计上，称这种形式为超标量体系（superscalar）结构，因为它可以在同一阶段同一时间做不同的几件事情。 应用程序阶段通常完成一些不在其他阶段执行的计算，如层次视锥裁剪等加速算法就可以在这里实现。 应用程序阶段的主要任务：在应用程序阶段的末端，将需要在屏幕上（具体形式取决于具体输入设备）显示出来绘制的几何体（也就是绘制图元(rendering primitives)：如点、线、矩形等）、以及摄像机位置输入到绘制管线的下一个阶段————几何阶段。 几何阶段 The Geometry Stage这个阶段用于处理几乎所有我们要绘制的几何相关的事情，比如决定画什么、怎么画、画在哪里等等（这一段主要在GPU上进行），因为事情太多，因此可以进一步细分成一个小的流水线： 顶点着色 Vertex Shading 投影 Projection 裁剪 Clipping 屏幕映射 Screen Mapping 顶点着色 Vertex Shading顶点着色 Vertex Shading 阶段主要的任务是： 计算顶点的位置 计算程序员要的顶点的输出数据例如：法线、纹理贴图坐标等等 一般来说, 模型大部分的着色是通过：计算光源作用于每个顶点的位置、法线信息的结果，存储时仅存储顶点处的颜色，然后在三角形上插值这些颜色来得到的。顶点着色器现在是一个更通用的部分, 专门用于设置与每个顶点关联的数据比如可以用于顶点绑定、图像变换等等。 在绘制到屏幕上之前，模型通常需要变换到若干不同的空间或坐标系中。模型变换的变换对象一般是模型的顶点和法线。物体的坐标称为模型坐标。世界空间是唯一的，所有的模型经过变换后都位于同一个空间中。 就像上文提到，应该仅对相机（或者视点）可以看到的模型进行绘制，而相机在世界空间中有一个位置和一个方向。为了方便投影和裁剪，必须对相机和所有的模型进行视点变换。变换的目的就是要把相机放在原点，然后进行视点校准，使其朝向 Z 轴负方向，y 轴指向上方,x 轴指向右边。在视点变换后，实际位置和方向就依赖于当前的 API。我们称上述空间为相机空间或者观察空间，下图显示了视点变换对相机和模型的影响。 为了产生逼真的场景，仅仅渲染形状和位置是远远不够的，我们需要对物体的外观进行建模。而物体经过建模，会得到对包括每个对象的材质，以及照射在对象上的任何光源的效果在内的一些刻画。且光照和材质可以用任意数量的方式，从简单的颜色描述到复杂的物理描述来模拟。 确定材质上的光照效果的这种操作被称为着色（shading），着色过程涉及在对象上的各个点处计算着色方程（shading equation）。通常，这些计算中的一些在几何阶段期间在模型的顶点上执行（vertex shading），而其他计算可以在每像素光栅化（per-pixel rasterization）期间执行。可以在每个顶点处存储各种材料数据，诸如点的位置，法线，颜色或计算着色方程所需的任何其它数字信息。顶点着色的结果（其可以是颜色，向量，纹理坐标或任何其他种类的着色数据）计算完成后，会被发送到光栅化阶段以进行插值操作。 着色计算通常认为是在世界空间中进行的。在实践中，有时需要将相关实体（诸如相机和光源）转换到一些其它空间（诸如模型或观察空间）并在那里执行计算，也可以得到正确的结果。这是因为如果着色过程中所有的实体变换到了相同的空间，着色计算中需要的诸如光源，相机和模型之间的相对关系是不会变的。 投影 Projection在光照处理之后，渲染系统就开始进行投影操作，即将视体变换到一个对角顶点分别是 (-1,-1,-1) 和 (1,1,1) 单位立方体（unit cube）内，这个单位立方体通常也被称为规范视域体（Canonical View Volume，CVV）。目前，主要有两种投影方法，即： 正交投影（orthographic projection，或称 parallel projection）：正交投影的可视体通常是一个矩形，正交投影可以把这个视体变换为单位立方体。正交投影的主要特性是平行线在变换之后彼此之间仍然保持平行，这种变换是平移与缩放的组合。 透视投影（perspective projection）：相比之下，透视投影比正交投影复杂一些。在这种投影中，越远离摄像机的物体，它在投影后看起来越小。更进一步来说，平行线将在地平线处会聚。透视投影的变换其实就是模拟人类感知物体的方式。 正交投影和透视投影都可以通过 4 x 4 的矩阵来实现，在任何一种变换之后，都可以认为模型位于归一化处理之后的设备坐标系中。虽然这些矩阵变换是从一个可视体变换到另一个，但它们仍被称为投影，因为在完成显示后，Z 坐标将不会再保存于的得到的投影图片中。通过这样的投影方法，就将模型从三维空间投影到了二维的空间中。 可选择的顶点处理这一部分是这个流水线上的可选项，不一定必须经历这个环节，是否启用取决于硬件的条件，这些环节彼此之间是相互独立的，按照顺序是： 曲面细分 tessellation：例如我们用三角形来描绘物体，理论上将三角形划分的越细物体也就越精细，如果这个物体离摄像头很远或者只在摄像头内出现一角，花费大量的三角形细分就比较浪费资源了。但在应用曲面细分，曲面就可以用适当的三角形数来生成。 几何元着色 geometry shading：这个阶段的作用和曲面细分类似，都可以生成新的顶点，但这是一个输出与输入的图元都受限的阶段，相应的也较为简单快速。最广泛应用这个阶段的地方可能是 生成粒子 例如烟火效果，每一点火星就好像一个点，几何元着色可以将这个点由一个面对观众的正方形（两个三角形组成）来表示，这更方便我们渲染图元。 流式输出 stream output：这个阶段让我将GPU当作几何引擎，这个阶段我们不是把数据传给剩余的流水线去处理，而是可以选择将它们输出到数组以进行进一步处理，这些数据可以给CPU，或者GPU自己，这个阶段在经常用于生成粒子，例如上面烟火的例子。 裁剪 Clipping只有当图元完全或部分存在于视体（也就是上文的规范视域体CVV）内部的时候，才需要将其发送到光栅化阶段，这个阶段可以把这些图元在屏幕上绘制出来。显然一个图元相对视体内部的位置，分为三种情况：完全位于内部、完全位于外部、部分位于内部。所以就要分情况进行处理： 当图元完全位于视体内部：那么它可以直接进行下一个阶段。 当图元完全位于视体外部：不会进入下一个阶段，可直接丢弃，因为它们无需进行渲染。 当图元部分位于视体内部：则需要对那些部分位于视体内的图元进行裁剪处理。对部分位于视体内部的图元进行裁剪操作，这就是裁剪过程存在的意义。裁剪过程见下图。 屏幕映射 Screen Mapping只有在视体内部经过裁剪的图元，以及之前完全位于视体内部的图元，才可以进入到屏幕映射阶段。进入到这个阶段时，坐标仍然是三维的（但显示状态在经过投影阶段后已经成了二维），每个图元的 x 和 y 坐标变换到了屏幕坐标系中，屏幕坐标系连同 z 坐标一起称为窗口坐标系。假定在一个窗口里对场景进行绘制，窗口的最小坐标为（x1，y1），最大坐标为（x2，y2），其中 x1 &lt; x2，y1 &lt; y2。屏幕映射首先进行平移，随后进行缩放，在映射过程中 z 坐标不受影响。新的 x 和 y 坐标称为屏幕坐标系，与 z 坐标一起（OpenGL下是[-1,+1]，DirectX下是[0,1]）进入光栅化阶段。如下图： 屏幕映射阶段的一个常见困惑是整型和浮点型的点值如何与像素坐标（或纹理坐标）进行关联。因为像素的中点可以定义在0.5处，因此索引在[0,9]之内的像素可以表示的浮点范围在[0.0,10.0)之内，这个转换可以简单的通过以下来转变：$$d=floor(c)$$$$c=d+0.5$$其中 d 是像素的整数索引, c 是像素内的浮点值。 光栅化阶段 The Rasterizer Stage给定经过变换和投影之后的顶点，颜色以及纹理坐标（均来自于几何阶段），给每个像素（Pixel）正确配色，以便正确绘制整幅图像。这个过个过程叫光珊化（rasterization）这个阶段主要分成两部分：三角形设定（Triangle Setup）也被称为图元装配和三角形遍历（Triangle Traversal），即从二维顶点所处的屏幕空间（所有顶点都包含 Z 值即深度值，及各种与相关的着色信息）到屏幕上的像素的转换。 三角形设定 Triangle Setup三角形设定阶段主要用来计算三角形表面差异和边缘方程等其他相关数据。该数据主要用于扫描转换（scan conversion），以及由几何阶段处理的各种着色数据的插值操作所用，该过程在专门为其设计的硬件上执行。 三角形遍历 Triangle Traversal在三角形遍历阶段将进行逐像素检查操作，检查该像素处的像素中心是否由三角形覆盖，而对于有三角形部分重合的像素，将在其重合部分生成片段（fragment）。找到哪些采样点或像素在三角形中的过程通常叫三角形遍历（TriangleTraversal）或扫描转换（scan conversion）。每个三角形片段的属性均由三个三角形顶点的数据插值而生成。这些属性包括片段的深度，以及来自几何阶段的着色数据。 像素处理 Pixel Processing在这个阶段，所有的像素已经都在一个三角形中或者是前面一系列处理之后的图元下，像素处理阶段被分为像素着色和融合两个子阶段 像素着色 Pixel Shading所有逐像素的着色计算都在像素着色阶段进行，使用插值得来的着色数据作为输入，输出结果为一种或多种将被传送到下一阶段的颜色信息。纹理贴图操作就是在这阶段进行的。像素着色阶段是在可编程 GPU 内执行的，在这一阶段有大量的技术可以使用，其中最常见，最重要的技术之一就是纹理贴图（Texturing）。纹理贴图在书的第六章会详细讲到。简单来说，纹理贴图就是将指定图片“贴”到指定物体上的过程。而指定的图片可以是一维，二维，或者三维的，其中，自然是二维图片最为常见。如下图所示： 融合 Merging每个像素的信息都储存在颜色缓冲器中，而颜色缓冲器是一个颜色的矩阵列（每种颜色包含红、绿、蓝三个分量）。融合阶段的主要任务是合成当前储存于缓冲器中的由之前的像素着色阶段产生的片段颜色。不像其它着色阶段，通常运行该阶段的 GPU 子单元并非完全可编程的，但其高度可配置，可支持多种特效。 这个阶段还负责可见性问题的处理。这意味着当绘制完整场景的时候，颜色缓冲器中应该还包含从相机视点处可以观察到的场景图元。对于大多数图形硬件来说，这个过程是通过 Z 缓冲（也称深度缓冲器）算法来实现的。Z 缓冲算法非常简单，具有 $O(n)$ 复杂度（n 是需要绘制的像素数量），只要对每个图元计算出相应的像素 z 值，就可以使用这种方法，大概内容是： Z 缓冲器和颜色缓冲器形状大小一样，每个像素都存储着一个 z 值，这个 z 值是从相机到最近图元之间的距离。 每次将一个图元绘制为相应像素时，需要计算像素位置处图元的 z 值，并与同一像素处的 z 缓冲器内容进行比较。 如果新计算出的 z 值，远远小于 Z 缓冲器中的 z 值，那么说明即将绘制的图元与相机的距离比原来距离相机最近的图元还要近。这样像素的 z 值和颜色就由当前图元对应的值和颜色进行更新。反之，若计算出的 z 值远远大于 z 缓冲器中的 z 值，那么 z 缓冲器和颜色缓冲器中的值就无需改变。 值得注意的是这个算法不能用于渲染半透明物体，原因我们会在往后的文章解释 颜色缓冲器用来存储颜色，z 缓冲器用来存储每个像素的 z 值，还有其他缓冲器可以用来过滤和捕获片段信息。 比如 alpha 通道（alpha channel）和颜色缓冲器联系在一起可以存储一个与每个像素相关的不透明值。可选的 alpha 测试可在深度测试执行前在传入片段上运行。片段的 alpha 值与参考值作某些特定的测试（如等于，大于等），如果片断未能通过测试，它将不再进行进一步的处理。alpha 测试经常用于不影响深度缓存的全透明片段的处理。 模板缓冲器（stencil buffer）是用于记录所呈现图元位置的离屏缓存。每个像素通常与占用 8 个位。图元可使用各种方法渲染到模板缓冲器中，而缓冲器中的内容可以控制颜色缓存和 Z 缓存的渲染。举个例子，假设在模版缓冲器中绘制出了一个实心圆形，那么可以使用一系列操作符来将后续的图元仅在圆形所出现的像素处绘制，类似一个 mask 的操作。模板缓冲器是制作特效的强大工具。而在管线末端的所有这些功能都叫做光栅操作（raster operations ，ROP）或混合操作（blend operations）可以将颜色缓冲区中当前的颜色与在三角形内处理的像素的颜色混合，这样可以实现半透明或者颜色累积的效果。 帧缓冲器（frame buffer）通常包含一个系统所具有的所有缓冲器，但有时也可以认为是颜色缓冲器和 Z 缓冲器的组合。 而当图元通过光栅化阶段之后，从相机视点处看到的东西就可以在荧幕上显示出来。为了避免观察者体验到对图元进行处理并发送到屏幕的过程，图形系统一般使用了双缓冲（double buffering）机制，这意味着屏幕绘制是在一个后置缓冲器（back buffer）中以离屏的方式进行的。一旦屏幕已在后置缓冲器中绘制，后置缓冲器中的内容就不断与已经在屏幕上显示过的前置缓冲器中的内容进行交换，当然，只有当不影响显示的时候才进行交换。","link":"/2019/04/28/Unity Shader 入门（一）：理论准备（一）/"}],"tags":[],"categories":[{"name":"Shader","slug":"Shader","link":"/categories/Shader/"}]}